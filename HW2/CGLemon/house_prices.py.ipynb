{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e01b83ad-2490-456a-a3e6-b72dddfdd163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob, os, io\n",
    "import math, random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import shap\n",
    "\n",
    "class FullyConnect(nn.Module):\n",
    "    def __init__(self, in_size,\n",
    "                       out_size,\n",
    "                       act=nn.Identity):\n",
    "        super(FullyConnect, self).__init__()\n",
    "        self.act = act\n",
    "        self.linear = nn.Linear(in_size, out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        o = self.linear(x)\n",
    "        return self.act(o)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            FullyConnect(in_size, 512, act=nn.ReLU()),\n",
    "            FullyConnect(512, 1024, act=nn.ReLU()),\n",
    "            FullyConnect(1024, 512, act=nn.ReLU()),\n",
    "            FullyConnect(512, out_size, act=nn.ReLU())\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        o = self.layers(x)\n",
    "        return o\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x_chunk, y_chunk):\n",
    "        self.x_chunk = x_chunk\n",
    "        self.y_chunk = y_chunk\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_chunk)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.x_chunk[idx], self.y_chunk[idx]\n",
    "        x = torch.from_numpy(x).float()\n",
    "        y = torch.from_numpy(y).float()\n",
    "        return x, y\n",
    "\n",
    "def load_all_csv():\n",
    "    train_name = os.path.join(\"kaggle\", \"train.csv\")\n",
    "    test_name = os.path.join(\"kaggle\", \"test.csv\")\n",
    "    train_df = pd.read_csv(train_name)\n",
    "    test_df = pd.read_csv(test_name)\n",
    "    return train_df, test_df\n",
    "\n",
    "def print_info(info):\n",
    "    for i, result in enumerate(info):\n",
    "        label = result[\"label\"]\n",
    "        items = result[\"items\"]\n",
    "        idx = i+1\n",
    "\n",
    "        out = str()\n",
    "        if result[\"is_obj\"]:\n",
    "            out += \"{}. {} -> {}\".format(idx, label, items)\n",
    "        else:\n",
    "            l = items[0]\n",
    "            h = items[-1]\n",
    "            out += \"{}. {} -> {}~{}\".format(idx, label,l,h)\n",
    "        if result[\"NaN\"]:\n",
    "            out += \", include {} NaN...\".format(result[\"NaN\"])\n",
    "        print(out)\n",
    "\n",
    "def get_data_format(info):\n",
    "    dformat = dict()\n",
    "\n",
    "    for result in info:\n",
    "        label = result[\"label\"]\n",
    "        if label == \"Id\":\n",
    "            continue\n",
    "\n",
    "        data = result[\"data\"]\n",
    "        items = result[\"items\"]\n",
    "        if result[\"is_obj\"]:\n",
    "            vmap = dict()\n",
    "            for i, v in enumerate(items):\n",
    "                vmap[v] = i\n",
    "            vmap[\"NaN\"] = len(items)\n",
    "            dformat[label] = {\n",
    "                \"type\" : \"one-hot\",\n",
    "                \"size\" : len(items) + 1,\n",
    "                \"map\" : vmap\n",
    "            }\n",
    "        else:\n",
    "            val_accm = 0\n",
    "            cnt = 0\n",
    "            for k, v in data.items():\n",
    "                val_accm += (k * v)\n",
    "                cnt += v\n",
    "            mean = val_accm/cnt\n",
    "            dformat[label] = {\n",
    "                \"type\" : \"scalar\",\n",
    "                \"mean\" : mean\n",
    "            }\n",
    "    return dformat\n",
    "\n",
    "def transfer_data(df, dformat):\n",
    "    labels = df.keys().values\n",
    "    x_chunk = list()\n",
    "    y_chunk = list()\n",
    "    for index, row in df.iterrows():\n",
    "        x = list()\n",
    "        y = list()\n",
    "        for label in labels:\n",
    "            if label == \"Id\":\n",
    "                continue\n",
    "            val = row[label]\n",
    "            fmt = dformat[label]\n",
    "\n",
    "            if fmt[\"type\"] == \"one-hot\":\n",
    "                if not type(val) == str and math.isnan(val):\n",
    "                    val = \"NaN\"\n",
    "                sub = [0] * fmt[\"size\"]\n",
    "                idx = fmt[\"map\"][val]\n",
    "                sub[idx] = 1\n",
    "            else:\n",
    "                if label == \"SalePrice\":\n",
    "                    sub = [val/fmt[\"mean\"]]\n",
    "                else:\n",
    "                    sub = [0] * 2\n",
    "                    if math.isnan(val):\n",
    "                        sub[0] = 1.\n",
    "                    else:\n",
    "                        sub[1] = val/fmt[\"mean\"]\n",
    "\n",
    "            if label == \"SalePrice\":\n",
    "                y.extend(sub)\n",
    "            else: \n",
    "                x.extend(sub)\n",
    "        x_chunk.append(x)\n",
    "        y_chunk.append(y)\n",
    "    x_chunk = np.array(x_chunk, dtype=np.float32)\n",
    "    y_chunk = np.array(y_chunk, dtype=np.float32)\n",
    "    return x_chunk, y_chunk\n",
    "\n",
    "def process_df_info(df):\n",
    "    labels = df.keys().values\n",
    "    info = list()\n",
    "\n",
    "    for label in labels:\n",
    "        result = {}\n",
    "        data_dict = {}\n",
    "        num_nan = 0\n",
    "\n",
    "        for v in df[label]:\n",
    "            if not type(v) == str and math.isnan(v):\n",
    "                num_nan += 1\n",
    "                continue\n",
    "\n",
    "            is_obj = type(v) == str\n",
    "            if not v in data_dict:\n",
    "                data_dict[v] = 1\n",
    "            else:\n",
    "                data_dict[v] += 1\n",
    "\n",
    "        result[\"label\"] = label\n",
    "        result[\"items\"] = sorted(list(data_dict.keys()))\n",
    "        result[\"data\"] = data_dict\n",
    "        result[\"NaN\"] = num_nan\n",
    "        result[\"is_obj\"] = is_obj\n",
    "        info.append(result)\n",
    "    return info\n",
    "\n",
    "def shuffle(x_chunk, y_chunk):\n",
    "    buf = list()\n",
    "    for x, y in zip(x_chunk, y_chunk):\n",
    "       buf.append((x,y))\n",
    "    random.shuffle(buf)\n",
    "\n",
    "    x_chunk_out, y_chunk_out = list(), list()\n",
    "    for x, y in buf:\n",
    "        x_chunk_out.append(x)\n",
    "        y_chunk_out.append(y)\n",
    "    x_chunk_out = np.array(x_chunk_out, dtype=np.float32)\n",
    "    y_chunk_out = np.array(y_chunk_out, dtype=np.float32)\n",
    "    return x_chunk_out, y_chunk_out\n",
    "\n",
    "def split(x_chunk, y_chunk, r=0.9):\n",
    "    x_chunk, y_chunk = shuffle(x_chunk, y_chunk)\n",
    "    size = round(len(y_chunk) * r) \n",
    "    train_x = x_chunk[:size]\n",
    "    train_y = y_chunk[:size]\n",
    "    test_x = x_chunk[size:]\n",
    "    test_y = y_chunk[size:]\n",
    "    return train_x, train_y, test_x, test_y\n",
    "    \n",
    "def sperate(x_chunk, features):\n",
    "    s = x_chunk.shape[0]\n",
    "    n = len(features)\n",
    "\n",
    "    x_chunk_out = np.zeros( (s, n) )\n",
    "\n",
    "    for i, x in enumerate(x_chunk):\n",
    "        for j, v in enumerate(features):\n",
    "            x_chunk_out[i][j] = x[j]\n",
    "    return x_chunk_out\n",
    "\n",
    "def test_performance(model, loader, device):\n",
    "    err, cnt = 0, 0\n",
    "    for idx, batch in enumerate(loader):\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        predict = model(x)\n",
    "\n",
    "        val = torch.mean(torch.abs(predict - y)).item()\n",
    "        err += val\n",
    "        cnt += 1\n",
    "    print(\"{:.4f}\".format(err/cnt))\n",
    "\n",
    "def print_results(model, loader, device, dformat):\n",
    "    pred_result = \"Id,SalePrice\\n\"\n",
    "    for idx, batch in enumerate(loader):\n",
    "        x, _ = batch\n",
    "        x = x.to(device)\n",
    "        predict = model(x)\n",
    "        mean = dformat[\"SalePrice\"][\"mean\"]\n",
    "        price = mean * predict.item()\n",
    "        pred_result += \"{},{:.4f}\\n\".format(idx+1461, price)\n",
    "    pred_result = pred_result[:-1]\n",
    "\n",
    "    df = pd.read_csv(io.StringIO(pred_result), sep=\",\", header=None)\n",
    "    print(df)\n",
    "\n",
    "def shap_explaine(model, x, device):\n",
    "    def model_pred(inputs):\n",
    "        pred = model(torch.from_numpy(inputs).float().to(device))\n",
    "        pred = pred.squeeze(-1)\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "        return pred\n",
    "\n",
    "    x100 = shap.utils.sample(x, 100)\n",
    "    explainer = shap.Explainer(model_pred, x100)\n",
    "    shap_values = explainer(x)\n",
    "\n",
    "    accm = np.zeros(shap_values[0].values.shape)\n",
    "    cnt = 0\n",
    "    for v in shap_values:\n",
    "        accm += v.values\n",
    "        cnt += 1\n",
    "    accm /= cnt\n",
    "\n",
    "    ordered_list = list()\n",
    "    for i, val in enumerate(accm):\n",
    "        ordered_list.append((i, val))\n",
    "\n",
    "    ordered_list.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    top_n = 5\n",
    "    best_features = list()\n",
    "\n",
    "    for i, val in ordered_list[:top_n]:\n",
    "        best_features.append(i)\n",
    "    return best_features\n",
    "\n",
    "def train(model, loader, device, max_steps=10000, filename=None):\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    MSE = nn.MSELoss()\n",
    "    loss_accm = 0\n",
    "\n",
    "    num_steps = 0\n",
    "    running = True\n",
    "\n",
    "    model.train()\n",
    "    while running:\n",
    "        for idx, batch in enumerate(loader):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            predict = model(x)\n",
    "            loss = MSE(predict, y)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "            num_steps += 1\n",
    "            loss_accm += loss.item()\n",
    "            if num_steps % 100 == 0:\n",
    "                print(\"{} -> loss: {}\".format(num_steps, loss_accm/100))\n",
    "                loss_accm = 0\n",
    "            if num_steps >= max_steps:\n",
    "                running = False\n",
    "                break\n",
    "    if filename is not None:\n",
    "        torch.save(model.to(torch.device(\"cpu\")).state_dict(), filename)\n",
    "        model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a51e415-7fd2-46c0-a8fa-67c46221c1ae",
   "metadata": {},
   "source": [
    "### step 1\n",
    "預先載入所有資料，並解析資料要使用的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46e28b68-1f7c-4a90-9b9d-6e950341a322",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, pred_df = load_all_csv()\n",
    "dformat = get_data_format(process_df_info(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d398d09f-0a50-4a10-96bd-932e2c1d91e5",
   "metadata": {},
   "source": [
    "### step 2\n",
    "準備訓練資料和測試集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89567635-56a6-40d9-86c0-4b0df026469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_chunk, y_chunk = transfer_data(train_df, dformat)\n",
    "train_x, train_y, test_x, test_y = split(x_chunk, y_chunk, r=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cc0f66-92d3-4f6b-8b78-81ef0917d922",
   "metadata": {},
   "source": [
    "### step 3\n",
    "建構模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adb1476e-411e-4355-b042-822af13bdf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu=False\n",
    "in_size = x_chunk.shape[1]\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if use_gpu:\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "model = Model(in_size, 1)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a048ef3e-beeb-4bb0-8263-23012c54728b",
   "metadata": {},
   "source": [
    "### step 4\n",
    "訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b1a94e-b0d9-4fa5-a992-b6c762c86277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 -> loss: 1.973600346148014\n",
      "200 -> loss: 0.017987582273781298\n",
      "300 -> loss: 0.01444465178065002\n",
      "400 -> loss: 0.01220392733812332\n",
      "500 -> loss: 0.00958479120861739\n",
      "600 -> loss: 0.00915604965761304\n",
      "700 -> loss: 0.007757087792269885\n",
      "800 -> loss: 0.0072080878494307395\n",
      "900 -> loss: 0.006740096926223487\n",
      "1000 -> loss: 0.006926962533034384\n",
      "1100 -> loss: 0.005554789211601019\n",
      "1200 -> loss: 0.005342402439564467\n",
      "1300 -> loss: 0.00573105345480144\n",
      "1400 -> loss: 0.004257637853734195\n",
      "1500 -> loss: 0.004286079367157072\n",
      "1600 -> loss: 0.0036889947368763387\n",
      "1700 -> loss: 0.004233237935695797\n",
      "1800 -> loss: 0.004060474396683275\n",
      "1900 -> loss: 0.0030484240362420676\n",
      "2000 -> loss: 0.0031210837874095887\n"
     ]
    }
   ],
   "source": [
    "dataset = MyDataset(train_x, train_y)\n",
    "loader = DataLoader(dataset, batch_size=512, num_workers=4, shuffle=True)\n",
    "train(model, loader, device, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d67182-2806-4109-be36-2de9b9f50ec7",
   "metadata": {},
   "source": [
    "### step 5\n",
    "測試模型性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a62fef4-c672-4558-8ea8-224edee8ae76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0993\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "dataset = MyDataset(test_x, test_y)\n",
    "loader = DataLoader(dataset, batch_size=32, num_workers=1, shuffle=True)\n",
    "test_performance(model, loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6395eb-912d-4f39-ab09-7caff84b2128",
   "metadata": {},
   "source": [
    "### step 6\n",
    "列印預測結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "662cbccb-0b04-426e-aff0-f47e7969f4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0            1\n",
      "0       Id    SalePrice\n",
      "1     1461  123700.4308\n",
      "2     1462  106780.7802\n",
      "3     1463  192093.6288\n",
      "4     1464  201683.0728\n",
      "...    ...          ...\n",
      "1455  2915   74745.2974\n",
      "1456  2916   66122.1415\n",
      "1457  2917  166005.3050\n",
      "1458  2918  110409.8119\n",
      "1459  2919  208373.1348\n",
      "\n",
      "[1460 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "x_chunk, y_chunk = transfer_data(pred_df, dformat)\n",
    "dataset = MyDataset(x_chunk, y_chunk)\n",
    "loader = DataLoader(dataset, batch_size=1, num_workers=1, shuffle=False)\n",
    "print_results(model, loader, device, dformat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397c08fe-93f6-4707-adf8-737ba55fea64",
   "metadata": {},
   "source": [
    "### step 7\n",
    "挑選五個特徵，並測試此模型性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c76ddc0e-31fa-43cf-8461-43717e9b2a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PermutationExplainer explainer: 1461it [06:20,  3.76it/s]                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 -> loss: 1.2177675366401672\n",
      "200 -> loss: 1.1919190967082978\n",
      "300 -> loss: 1.1898518073558808\n",
      "400 -> loss: 1.1911892294883728\n",
      "500 -> loss: 1.1916122031211853\n",
      "600 -> loss: 1.1925452291965484\n",
      "700 -> loss: 1.187481814622879\n",
      "800 -> loss: 1.1939857506752014\n",
      "900 -> loss: 1.1926264333724976\n",
      "1000 -> loss: 1.188985254764557\n",
      "1100 -> loss: 1.1900306797027589\n",
      "1200 -> loss: 1.1902716207504271\n",
      "1300 -> loss: 1.1880878520011902\n",
      "1400 -> loss: 1.1939019668102264\n",
      "1500 -> loss: 1.190867644548416\n",
      "1600 -> loss: 1.1919747710227966\n",
      "1700 -> loss: 1.190871832370758\n",
      "1800 -> loss: 1.1920729625225066\n",
      "1900 -> loss: 1.1896206843852997\n",
      "2000 -> loss: 1.1923628509044648\n",
      "0.9967\n"
     ]
    }
   ],
   "source": [
    "x_chunk, y_chunk = transfer_data(train_df, dformat)\n",
    "best_features = shap_explaine(model, x_chunk, device)\n",
    "\n",
    "model_special = Model(5, 1)\n",
    "model_special = model_special.to(device)\n",
    "\n",
    "x_chunk = sperate(x_chunk, best_features)\n",
    "train_x, train_y, test_x, test_y = split(x_chunk, y_chunk, r=0.9)\n",
    "dataset = MyDataset(train_x, train_y)\n",
    "loader = DataLoader(dataset, batch_size=512, num_workers=4, shuffle=True)\n",
    "\n",
    "train(model_special, loader, device, 2000)\n",
    "model_special.eval()\n",
    "dataset = MyDataset(test_x, test_y)\n",
    "loader = DataLoader(dataset, batch_size=32, num_workers=1, shuffle=True)\n",
    "test_performance(model_special, loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeed2548-43cf-4a4f-9cc5-d814bec4c971",
   "metadata": {},
   "source": [
    "### step 8\n",
    "列印此模型預測結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ce7ae77-59f4-4fe6-9f70-9ac58e449f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0          1\n",
      "0       Id  SalePrice\n",
      "1     1461     0.0000\n",
      "2     1462     0.0000\n",
      "3     1463     0.0000\n",
      "4     1464     0.0000\n",
      "...    ...        ...\n",
      "1455  2915     0.0000\n",
      "1456  2916     0.0000\n",
      "1457  2917     0.0000\n",
      "1458  2918     0.0000\n",
      "1459  2919     0.0000\n",
      "\n",
      "[1460 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "x_chunk, y_chunk = transfer_data(pred_df, dformat)\n",
    "x_chunk = sperate(x_chunk, best_features)\n",
    "dataset = MyDataset(x_chunk, y_chunk)\n",
    "loader = DataLoader(dataset, batch_size=1, num_workers=1, shuffle=False)\n",
    "print_results(model_special, loader, device, dformat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e33242c-2620-40e3-845e-ec38f0141c31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
