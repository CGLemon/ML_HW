{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef1f8aeb-c692-4ac3-91cc-25a128f4a03f",
   "metadata": {},
   "source": [
    "# 導入所有必要的程式庫和宣告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4ef0833-9ab0-4082-9cd5-b6269a771092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "TARGET_SIZE = (28, 28)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409ac14b-e331-4e2a-b66a-86738a79b0b7",
   "metadata": {},
   "source": [
    "## 載入 MNIST 訓練資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a301a07b-5b55-415c-8fc4-d4a70a06680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Resize(size=TARGET_SIZE)])\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "class SubDataset(Dataset):\n",
    "    LABEL_TO_INDEX = {\n",
    "        1: 0, 3: 1, 5: 2, 7: 3\n",
    "    }\n",
    "    INDEX_TO_LABEL = [\n",
    "        1, 3, 5, 7\n",
    "    ]\n",
    "\n",
    "    def __init__(self, full_dataset):\n",
    "        self._data_pair = list()\n",
    "        for data in full_dataset:\n",
    "            img, label = data\n",
    "            if label in self.LABEL_TO_INDEX.keys():\n",
    "                self._data_pair.append((img, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data_pair)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self._data_pair[idx]\n",
    "        return img, self.LABEL_TO_INDEX[label]\n",
    "\n",
    "class AbnormDataset(Dataset):\n",
    "    NORM_LABEL = [\n",
    "        1, 3, 5, 7\n",
    "    ]\n",
    "\n",
    "    def __init__(self, full_dataset):\n",
    "        self._data_pair = list()\n",
    "        for data in full_dataset:\n",
    "            img, label = data\n",
    "            self._data_pair.append((img, label not in self.NORM_LABEL))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data_pair)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, abnorm = self._data_pair[idx]\n",
    "        return img, abnorm\n",
    "\n",
    "t_subset = SubDataset(trainset)\n",
    "t_loader = torch.utils.data.DataLoader(t_subset, batch_size=64, shuffle=True)\n",
    "v_subset = SubDataset(testset)\n",
    "v_loader = torch.utils.data.DataLoader(v_subset, batch_size=64, shuffle=True)\n",
    "a_dataset = AbnormDataset(testset)\n",
    "a_loader = torch.utils.data.DataLoader(a_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1caf847-b576-495b-a4f2-2c98ebdc959e",
   "metadata": {},
   "source": [
    "## 建構用於分類網路（第一小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4082ac2-2df9-46eb-9034-dce6428c5763",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnect(nn.Module):\n",
    "    def __init__(self, in_size,\n",
    "                       out_size,\n",
    "                       activation=None):\n",
    "        super().__init__()\n",
    "        self.act = activation\n",
    "        self.linear = nn.Linear(\n",
    "            in_size,\n",
    "            out_size,\n",
    "            bias=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        if not self.act is None:\n",
    "            x = self.act(x)\n",
    "        return x\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels,\n",
    "                       out_channels,\n",
    "                       kernel_size,\n",
    "                       activation=None):\n",
    "        super().__init__()\n",
    "        self.act = activation\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            padding=\"same\",\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(\n",
    "            out_channels,\n",
    "            eps=1e-5\n",
    "        )\n",
    "        nn.init.kaiming_normal_(self.conv.weight,\n",
    "                                mode=\"fan_out\",\n",
    "                                nonlinearity=\"relu\")\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        if not self.act is None:\n",
    "            x = self.act(x)\n",
    "        return x\n",
    "\n",
    "class ClassifyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClassifyNetwork, self).__init__()\n",
    "        self.img_size = TARGET_SIZE\n",
    "\n",
    "        self.body = nn.Sequential(\n",
    "            ConvBlock(1, 32, 7, nn.SiLU()),\n",
    "            ConvBlock(32, 32, 3, nn.SiLU()),\n",
    "            ConvBlock(32, 2, 3, nn.SiLU())\n",
    "        )\n",
    "\n",
    "        h, w = self.img_size\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            FullyConnect(h * w * 2, 128, nn.SiLU()),\n",
    "            nn.Dropout(p=0.1),\n",
    "            FullyConnect(128, 4),\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.body(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def get_prob(self, x):\n",
    "        x = self.forward(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc95aaf7-5565-49f9-afc2-75bce89c3aaf",
   "metadata": {},
   "source": [
    "## 訓練 MNIST 分類網路（第一小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9bc0645-6306-4a70-9654-7672332ccefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 -> loss: 0.0812, acc: 98.84%\n",
      "epoch 2 -> loss: 0.0281, acc: 99.34%\n",
      "epoch 3 -> loss: 0.0206, acc: 99.51%\n",
      "epoch 4 -> loss: 0.0164, acc: 99.63%\n",
      "epoch 5 -> loss: 0.0127, acc: 99.19%\n",
      "epoch 6 -> loss: 0.0116, acc: 99.63%\n",
      "epoch 7 -> loss: 0.0087, acc: 99.43%\n",
      "epoch 8 -> loss: 0.0078, acc: 99.58%\n",
      "epoch 9 -> loss: 0.0062, acc: 99.36%\n",
      "epoch 10 -> loss: 0.0070, acc: 99.53%\n"
     ]
    }
   ],
   "source": [
    "classify_net = ClassifyNetwork()\n",
    "classify_net = classify_net.to(device)\n",
    "\n",
    "cross_entroy = nn.CrossEntropyLoss()\n",
    "opt = optim.SGD(classify_net.parameters(),\n",
    "                lr=0.01,\n",
    "                momentum=0.9,\n",
    "                nesterov=True,\n",
    "                weight_decay=0.001)\n",
    "\n",
    "running_loss = list()\n",
    "for e in range(10):\n",
    "    classify_net.train()\n",
    "    for imgs, labels in t_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        opt.zero_grad()\n",
    "        loss = cross_entroy(classify_net(imgs), labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        running_loss.append(loss.item())\n",
    "        if len(running_loss) > 500:\n",
    "            running_loss.pop(0)\n",
    "\n",
    "    classify_net.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for imgs, labels in v_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = classify_net(imgs)\n",
    "            _, pred = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (pred == labels).sum().item()\n",
    "        print(\"epoch {} -> loss: {:.4f}, acc: {:.2f}%\".format(\n",
    "                  e+1, sum(running_loss)/len(running_loss), 100.0 * correct/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b413b060-7da7-4056-b5c3-c953c111d0c7",
   "metadata": {},
   "source": [
    "## 測試分類網路的檢測性能（第一小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f383ed8b-fe2b-412a-9a03-3d40b5a01ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.08%\n"
     ]
    }
   ],
   "source": [
    "classify_net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for imgs, abnorm in a_loader:\n",
    "        imgs, abnorm = imgs.to(device), abnorm.to(device)\n",
    "        prob, _ = torch.max(classify_net.get_prob(imgs), dim=1)\n",
    "        pred_abnorm = torch.where(prob < 0.99, True, False)\n",
    "        correct += (pred_abnorm == abnorm).sum().item()\n",
    "        total += abnorm.size(0)\n",
    "    classify_net_acc = correct/total\n",
    "    print(\"Accuracy: {:.2f}%\".format(100 * classify_net_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c2743f-cd5f-440a-88fa-06fab918c177",
   "metadata": {},
   "source": [
    "## 建構 AutoEncoder 的網路（第二小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73733600-a2a5-444d-907a-6e01437d3c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, ctx_size):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.img_size = TARGET_SIZE\n",
    "        h, w = self.img_size\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            FullyConnect(w * h, 256, nn.SiLU()),\n",
    "            FullyConnect(256, 128, nn.SiLU()),\n",
    "            nn.Dropout(p=0.1),\n",
    "            FullyConnect(128, ctx_size, nn.SiLU()),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            FullyConnect(ctx_size, 128, nn.SiLU()),\n",
    "            nn.Dropout(p=0.1),\n",
    "            FullyConnect(128, 256, nn.SiLU()),\n",
    "            FullyConnect(256, w * h, nn.Sigmoid())\n",
    "        )\n",
    "\n",
    "    def encode(self, img):\n",
    "        b, _, _, _ = img.shape\n",
    "        h, w = self.img_size\n",
    "        img = torch.reshape(img, (b, h * w))\n",
    "        ctx = self.encoder(img)\n",
    "        return ctx\n",
    "\n",
    "    def decode(self, ctx):\n",
    "        b, _ = ctx.shape\n",
    "        h, w = self.img_size\n",
    "        img = self.decoder(ctx)\n",
    "        img = torch.reshape(img, (b, 1, h, w))\n",
    "        return img\n",
    "\n",
    "    def forward(self, x):\n",
    "        ctx = self.encode(x)\n",
    "        x = self.decode(ctx)\n",
    "        return x\n",
    "\n",
    "def train_auto_encoder(net, device, loader):\n",
    "    net = net.to(device)\n",
    "    net.train()\n",
    "    bce_loss = nn.BCELoss()\n",
    "    opt = optim.Adam(net.parameters(),\n",
    "                     lr=0.002,\n",
    "                     weight_decay=0.0)\n",
    "\n",
    "    running_loss = list()\n",
    "\n",
    "    for e in range(100):\n",
    "        for imgs, _ in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            opt.zero_grad()\n",
    "            loss = bce_loss(net(imgs), imgs)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            running_loss.append(loss.item())\n",
    "            if len(running_loss) > 500:\n",
    "                running_loss.pop(0)\n",
    "        if (e+1) % 20 == 0:\n",
    "            print(\"epoch {} -> loss: {:.4f}\".format(\n",
    "                      e+1, sum(running_loss)/len(running_loss)))\n",
    "    return net\n",
    "\n",
    "def compute_thres(net, device, loader):\n",
    "    net.eval()\n",
    "    bce_loss_without_reduction = nn.BCELoss(reduction='none')\n",
    "    item_loss = torch.zeros(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, _ in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            loss = bce_loss_without_reduction(net(imgs), imgs)\n",
    "            loss = torch.flatten(loss, start_dim=1)\n",
    "            loss = torch.mean(loss, dim=1)\n",
    "            item_loss = torch.cat((item_loss, loss), 0)\n",
    "\n",
    "    item_loss = item_loss.detach().cpu().numpy()\n",
    "    mean = np.mean(item_loss)\n",
    "    std = np.std(item_loss)\n",
    "    thres = mean + 1.2 * std\n",
    "    return thres\n",
    "\n",
    "def compute_acc(net, device, loader, thres):\n",
    "    bce_loss_without_reduction = nn.BCELoss(reduction='none')\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for imgs, abnorm in loader:\n",
    "            imgs, abnorm = imgs.to(device), abnorm.to(device)\n",
    "            loss = bce_loss_without_reduction(ae_net(imgs), imgs)\n",
    "            loss = torch.flatten(loss, start_dim=1)\n",
    "            loss = torch.mean(loss, dim=1)\n",
    "            pred_abnorm = torch.where(loss > thres, True, False)\n",
    "            correct += (pred_abnorm == abnorm).sum().item()\n",
    "            total += abnorm.size(0)\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155784e9-abde-4218-9053-4c9907214fd6",
   "metadata": {},
   "source": [
    "## 訓練 AutoEncoder 的網路（第二小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff32ba0f-03be-4922-85bf-767d6a4ac543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 -> loss: 0.1421\n",
      "epoch 40 -> loss: 0.1385\n",
      "epoch 60 -> loss: 0.1370\n",
      "epoch 80 -> loss: 0.1358\n",
      "epoch 100 -> loss: 0.1353\n"
     ]
    }
   ],
   "source": [
    "ae_net = AutoEncoder(2)\n",
    "ae_net = ae_net.to(device)\n",
    "ae_net = train_auto_encoder(ae_net, device, t_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2ca8ea-e375-4e40-a611-b8c863b869fc",
   "metadata": {},
   "source": [
    "## 測試 AutoEncoder 的檢測性能（第二小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "577de0ca-6c42-4f00-8278-e4f9f3ac5d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.12%\n"
     ]
    }
   ],
   "source": [
    "thres = compute_thres(ae_net, device, t_loader)\n",
    "ae_net_acc = compute_acc(ae_net, device, a_loader, thres)\n",
    "print(\"Accuracy: {:.2f}%\".format(100 * ae_net_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b234de-1403-4e8e-9a55-01d7af8d271d",
   "metadata": {},
   "source": [
    "## 建構 Denoising AutoEncoder 的網路（第三小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ada95a6-a956-4bf0-8b20-0247ebe4601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoisingAutoEncoder(AutoEncoder):\n",
    "    def __init__(self, ctx_size):\n",
    "        super(DenoisingAutoEncoder, self).__init__(ctx_size)\n",
    "\n",
    "    def add_noise(self, img, noise_factor=0.25):\n",
    "        noise = torch.randn_like(img) * noise_factor\n",
    "        noisy_img = img + noise\n",
    "        return torch.clamp(noisy_img, 0., 1.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.add_noise(x)\n",
    "        ctx = self.encode(x)\n",
    "        x = self.decode(ctx)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a0d281-7b62-48d4-9e43-6798df9faf02",
   "metadata": {},
   "source": [
    "## 訓練 Denoising AutoEncoder 的網路（第三小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94cc920a-6a04-4699-b460-80bfd7f09da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 -> loss: 0.1452\n",
      "epoch 40 -> loss: 0.1430\n",
      "epoch 60 -> loss: 0.1413\n",
      "epoch 80 -> loss: 0.1405\n",
      "epoch 100 -> loss: 0.1400\n"
     ]
    }
   ],
   "source": [
    "dae_net = DenoisingAutoEncoder(2)\n",
    "dae_net = dae_net.to(device)\n",
    "dae_net = train_auto_encoder(dae_net, device, t_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf349544-6594-492c-be1f-43c5f3d8b345",
   "metadata": {},
   "source": [
    "## 測試 Denoising AutoEncoder 的檢測性能（第三小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16803b62-f83a-458f-9945-06ee05adc0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.16%\n"
     ]
    }
   ],
   "source": [
    "thres = compute_thres(dae_net, device, t_loader)\n",
    "dae_net_acc = compute_acc(dae_net, device, a_loader, thres)\n",
    "print(\"Accuracy: {:.2f}%\".format(100 * dae_net_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30be95be-b008-420b-85ea-406d864710db",
   "metadata": {},
   "source": [
    "## 建構 Variational AutoEncoder 的網路（第四小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad1a9041-2206-4528-be00-1d6e4814d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(nn.Module):\n",
    "    def __init__(self, ctx_size):\n",
    "        super(VariationalAutoEncoder, self).__init__()\n",
    "        self.img_size = TARGET_SIZE\n",
    "        self.ctx_size = ctx_size\n",
    "        h, w = self.img_size\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            FullyConnect(w * h, 256, nn.SiLU()),\n",
    "            FullyConnect(256, 128, nn.SiLU()),\n",
    "            nn.Dropout(p=0.1),\n",
    "            FullyConnect(128, 2 * ctx_size, nn.SiLU()),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            FullyConnect(ctx_size, 128, nn.SiLU()),\n",
    "            nn.Dropout(p=0.1),\n",
    "            FullyConnect(128, 256, nn.SiLU()),\n",
    "            FullyConnect(256, w * h, nn.Sigmoid())\n",
    "        )\n",
    "\n",
    "    def encode(self, img):\n",
    "        b, _, _, _ = img.shape\n",
    "        h, w = self.img_size\n",
    "        img = torch.reshape(img, (b, h * w))\n",
    "        x = self.encoder(img)\n",
    "        mu, logvar = torch.split(x, self.ctx_size, dim=1)\n",
    "        return mu, logvar\n",
    "\n",
    "    def decode(self, ctx):\n",
    "        b, _ = ctx.shape\n",
    "        h, w = self.img_size\n",
    "        img = self.decoder(ctx)\n",
    "        img = torch.reshape(img, (b, 1, h, w))\n",
    "        return img\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        x = self.reparameterize(mu, logvar)\n",
    "        x = self.decode(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0d3c14-a056-4b28-aa7f-dc0f77c34c12",
   "metadata": {},
   "source": [
    "## 訓練 Variational AutoEncoder 的網路（第四小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2abb875f-9bf9-44c3-8922-91e462a97521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 -> loss: 0.1436\n",
      "epoch 40 -> loss: 0.1402\n",
      "epoch 60 -> loss: 0.1386\n",
      "epoch 80 -> loss: 0.1378\n",
      "epoch 100 -> loss: 0.1381\n"
     ]
    }
   ],
   "source": [
    "vae_net = VariationalAutoEncoder(2)\n",
    "vae_net = vae_net.to(device)\n",
    "vae_net = train_auto_encoder(vae_net, device, t_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e57d38-f2b6-49d8-b7fe-2d0473682ba6",
   "metadata": {},
   "source": [
    "## 測試 Variational AutoEncoder 的檢測性能（第四小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd1a8548-cd82-4ff7-8043-4c16b73147bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.99%\n"
     ]
    }
   ],
   "source": [
    "thres = compute_thres(vae_net, device, t_loader)\n",
    "vae_net_acc = compute_acc(vae_net, device, a_loader, thres)\n",
    "print(\"Accuracy: {:.2f}%\".format(100 * vae_net_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52b103d-4c35-4c6e-898c-96763cba6243",
   "metadata": {},
   "source": [
    "## 使用 Variational AutoEncoder 的結果做異常檢測（第五小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ddcce0f-0968-42d4-8bf1-a478e17799a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m imgs, labels \u001b[38;5;129;01min\u001b[39;00m a_loader:\n\u001b[0;32m----> 6\u001b[0m         ctx \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241m.\u001b[39mcontext(imgs)\n\u001b[1;32m      7\u001b[0m         ctx_items \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((ctx_items, ctx), \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      8\u001b[0m ctx_items \u001b[38;5;241m=\u001b[39m ctx_items\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "vae_net.eval()\n",
    "vae_net = vae_net.to(torch.device(\"cpu\"))\n",
    "items = torch.zeros(0)\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in a_loader:\n",
    "        ctx = vae_net.encode(imgs)\n",
    "        ctx_items = torch.cat((ctx_items, ctx), 0)\n",
    "ctx_items = ctx_items.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b9c6f5-56c5-4880-b6e2-befe29b8fbab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dca8c0-7c59-4039-96ed-1d66a3b0d2bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6f7a4f-c455-4ea4-9a89-20b76c4e03da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a803bb-12b7-4d1a-9574-8128ca4025d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
