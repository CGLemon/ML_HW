{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef1f8aeb-c692-4ac3-91cc-25a128f4a03f",
   "metadata": {},
   "source": [
    "## 導入所有必要的程式庫和宣告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4ef0833-9ab0-4082-9cd5-b6269a771092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "TARGET_SIZE = (28, 28)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409ac14b-e331-4e2a-b66a-86738a79b0b7",
   "metadata": {},
   "source": [
    "## 載入 MNIST 訓練資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a301a07b-5b55-415c-8fc4-d4a70a06680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Resize(size=TARGET_SIZE)])\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "class SubDataset(Dataset):\n",
    "    LABEL_TO_INDEX = {\n",
    "        1: 0, 3: 1, 5: 2, 7: 3\n",
    "    }\n",
    "    INDEX_TO_LABEL = [\n",
    "        1, 3, 5, 7\n",
    "    ]\n",
    "\n",
    "    def __init__(self, full_dataset):\n",
    "        self._data_pair = list()\n",
    "        for data in full_dataset:\n",
    "            img, label = data\n",
    "            if label in self.LABEL_TO_INDEX.keys():\n",
    "                self._data_pair.append((img, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data_pair)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self._data_pair[idx]\n",
    "        return img, self.LABEL_TO_INDEX[label]\n",
    "\n",
    "class AbnormDataset(Dataset):\n",
    "    NORM_LABEL = [\n",
    "        1, 3, 5, 7\n",
    "    ]\n",
    "\n",
    "    def __init__(self, full_dataset):\n",
    "        self._data_pair = list()\n",
    "        for data in full_dataset:\n",
    "            img, label = data\n",
    "            self._data_pair.append((img, label not in self.NORM_LABEL))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data_pair)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, abnorm = self._data_pair[idx]\n",
    "        return img, abnorm\n",
    "\n",
    "t_subset = SubDataset(trainset)\n",
    "t_loader = torch.utils.data.DataLoader(t_subset, batch_size=64, shuffle=True)\n",
    "v_subset = SubDataset(testset)\n",
    "v_loader = torch.utils.data.DataLoader(v_subset, batch_size=64, shuffle=True)\n",
    "r_dataset = AbnormDataset(trainset)\n",
    "r_loader = torch.utils.data.DataLoader(r_dataset, batch_size=64, shuffle=True)\n",
    "a_dataset = AbnormDataset(testset)\n",
    "a_loader = torch.utils.data.DataLoader(a_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1caf847-b576-495b-a4f2-2c98ebdc959e",
   "metadata": {},
   "source": [
    "## 建構用於分類網路（第一小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4082ac2-2df9-46eb-9034-dce6428c5763",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnect(nn.Module):\n",
    "    def __init__(self, in_size,\n",
    "                       out_size,\n",
    "                       activation=None):\n",
    "        super().__init__()\n",
    "        self.act = activation\n",
    "        self.linear = nn.Linear(\n",
    "            in_size,\n",
    "            out_size,\n",
    "            bias=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        if not self.act is None:\n",
    "            x = self.act(x)\n",
    "        return x\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels,\n",
    "                       out_channels,\n",
    "                       kernel_size,\n",
    "                       activation=None):\n",
    "        super().__init__()\n",
    "        self.act = activation\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            padding=\"same\",\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(\n",
    "            out_channels,\n",
    "            eps=1e-5\n",
    "        )\n",
    "        nn.init.kaiming_normal_(self.conv.weight,\n",
    "                                mode=\"fan_out\",\n",
    "                                nonlinearity=\"relu\")\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        if not self.act is None:\n",
    "            x = self.act(x)\n",
    "        return x\n",
    "\n",
    "class ClassifyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClassifyNetwork, self).__init__()\n",
    "        self.img_size = TARGET_SIZE\n",
    "\n",
    "        self.body = nn.Sequential(\n",
    "            ConvBlock(1, 32, 7, nn.SiLU()),\n",
    "            ConvBlock(32, 32, 3, nn.SiLU()),\n",
    "            ConvBlock(32, 2, 3, nn.SiLU())\n",
    "        )\n",
    "\n",
    "        h, w = self.img_size\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            FullyConnect(h * w * 2, 128, nn.SiLU()),\n",
    "            nn.Dropout(p=0.1),\n",
    "            FullyConnect(128, 4),\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.body(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def get_prob(self, x):\n",
    "        x = self.forward(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc95aaf7-5565-49f9-afc2-75bce89c3aaf",
   "metadata": {},
   "source": [
    "## 訓練 MNIST 分類網路（第一小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9bc0645-6306-4a70-9654-7672332ccefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 -> loss: 0.0895, acc: 99.21%\n",
      "epoch 2 -> loss: 0.0260, acc: 99.48%\n",
      "epoch 3 -> loss: 0.0185, acc: 99.38%\n",
      "epoch 4 -> loss: 0.0160, acc: 99.46%\n",
      "epoch 5 -> loss: 0.0129, acc: 99.66%\n",
      "epoch 6 -> loss: 0.0109, acc: 99.58%\n",
      "epoch 7 -> loss: 0.0089, acc: 99.61%\n",
      "epoch 8 -> loss: 0.0084, acc: 99.68%\n",
      "epoch 9 -> loss: 0.0070, acc: 99.63%\n",
      "epoch 10 -> loss: 0.0062, acc: 99.29%\n"
     ]
    }
   ],
   "source": [
    "classify_net = ClassifyNetwork()\n",
    "classify_net = classify_net.to(device)\n",
    "\n",
    "cross_entroy = nn.CrossEntropyLoss()\n",
    "opt = optim.SGD(classify_net.parameters(),\n",
    "                lr=0.01,\n",
    "                momentum=0.9,\n",
    "                nesterov=True,\n",
    "                weight_decay=0.001)\n",
    "\n",
    "running_loss = list()\n",
    "for e in range(10):\n",
    "    classify_net.train()\n",
    "    for imgs, labels in t_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        opt.zero_grad()\n",
    "        loss = cross_entroy(classify_net(imgs), labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        running_loss.append(loss.item())\n",
    "        if len(running_loss) > 500:\n",
    "            running_loss.pop(0)\n",
    "\n",
    "    classify_net.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for imgs, labels in v_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = classify_net(imgs)\n",
    "            _, pred = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (pred == labels).sum().item()\n",
    "        print(\"epoch {} -> loss: {:.4f}, acc: {:.2f}%\".format(\n",
    "                  e+1, sum(running_loss)/len(running_loss), 100.0 * correct/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b413b060-7da7-4056-b5c3-c953c111d0c7",
   "metadata": {},
   "source": [
    "## 測試分類網路的檢測性能（第一小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f383ed8b-fe2b-412a-9a03-3d40b5a01ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test... current best accuracy is 78.80%\n",
      "Test... current best accuracy is 78.86%\n",
      "Test... current best accuracy is 78.88%\n",
      "Test... current best accuracy is 78.91%\n",
      "Test... current best accuracy is 78.98%\n",
      "Test... current best accuracy is 79.00%\n",
      "Test... current best accuracy is 79.06%\n",
      "Test... current best accuracy is 79.12%\n",
      "Test... current best accuracy is 79.16%\n",
      "Test... current best accuracy is 79.22%\n",
      "Test... current best accuracy is 79.26%\n",
      "Test... current best accuracy is 79.33%\n",
      "Test... current best accuracy is 79.36%\n",
      "Test... current best accuracy is 79.40%\n",
      "Test... current best accuracy is 79.50%\n",
      "Test... current best accuracy is 79.54%\n",
      "Test... current best accuracy is 79.59%\n",
      "Test... current best accuracy is 79.62%\n",
      "Test... current best accuracy is 79.68%\n",
      "Test... current best accuracy is 79.74%\n",
      "Test... current best accuracy is 79.86%\n",
      "Test... current best accuracy is 79.97%\n",
      "Test... current best accuracy is 80.03%\n",
      "Test... current best accuracy is 80.11%\n",
      "Test... current best accuracy is 80.16%\n",
      "Test... current best accuracy is 80.20%\n",
      "Test... current best accuracy is 80.22%\n",
      "Test... current best accuracy is 80.29%\n",
      "Test... current best accuracy is 80.33%\n",
      "Test... current best accuracy is 80.34%\n",
      "Test... current best accuracy is 80.40%\n",
      "Test... current best accuracy is 80.48%\n",
      "Test... current best accuracy is 80.55%\n",
      "Test... current best accuracy is 80.60%\n",
      "Test... current best accuracy is 80.68%\n",
      "Test... current best accuracy is 80.75%\n",
      "Test... current best accuracy is 80.86%\n",
      "Test... current best accuracy is 80.93%\n",
      "Test... current best accuracy is 80.98%\n",
      "Test... current best accuracy is 81.08%\n",
      "Test... current best accuracy is 81.13%\n",
      "Test... current best accuracy is 81.20%\n",
      "Test... current best accuracy is 81.34%\n",
      "Test... current best accuracy is 81.37%\n",
      "Test... current best accuracy is 81.38%\n",
      "Test... current best accuracy is 81.41%\n",
      "Test... current best accuracy is 81.49%\n",
      "Test... current best accuracy is 81.59%\n",
      "Test... current best accuracy is 81.65%\n",
      "Test... current best accuracy is 81.77%\n",
      "Test... current best accuracy is 81.82%\n",
      "Test... current best accuracy is 81.88%\n",
      "Test... current best accuracy is 82.01%\n",
      "Test... current best accuracy is 82.05%\n",
      "Test... current best accuracy is 82.09%\n",
      "Test... current best accuracy is 82.19%\n",
      "Test... current best accuracy is 82.29%\n",
      "Test... current best accuracy is 82.34%\n",
      "Test... current best accuracy is 82.44%\n",
      "Test... current best accuracy is 82.54%\n",
      "Test... current best accuracy is 82.65%\n",
      "Test... current best accuracy is 82.72%\n",
      "Test... current best accuracy is 82.78%\n",
      "Test... current best accuracy is 82.79%\n",
      "Test... current best accuracy is 82.84%\n",
      "Test... current best accuracy is 82.99%\n",
      "Test... current best accuracy is 83.11%\n",
      "Test... current best accuracy is 83.19%\n",
      "Test... current best accuracy is 83.26%\n",
      "Test... current best accuracy is 83.39%\n",
      "Test... current best accuracy is 83.41%\n",
      "Test... current best accuracy is 83.50%\n",
      "Test... current best accuracy is 83.59%\n",
      "Test... current best accuracy is 83.71%\n",
      "Test... current best accuracy is 83.78%\n",
      "Test... current best accuracy is 83.87%\n",
      "Test... current best accuracy is 84.04%\n",
      "Test... current best accuracy is 84.15%\n",
      "Test... current best accuracy is 84.20%\n",
      "Test... current best accuracy is 84.33%\n",
      "Test... current best accuracy is 84.38%\n",
      "Test... current best accuracy is 84.44%\n",
      "Test... current best accuracy is 84.48%\n",
      "Test... current best accuracy is 84.49%\n",
      "Test... current best accuracy is 84.61%\n",
      "Test... current best accuracy is 84.64%\n",
      "Test... current best accuracy is 84.72%\n",
      "Best accuracy: 84.72%\n"
     ]
    }
   ],
   "source": [
    "classify_net.eval()\n",
    "best_classify_net_acc = 0\n",
    "for i in range(100):\n",
    "    thres = 0.995 + 0.005 * (i/100)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for imgs, abnorm in a_loader:\n",
    "            imgs, abnorm = imgs.to(device), abnorm.to(device)\n",
    "            prob, _ = torch.max(classify_net.get_prob(imgs), dim=1)\n",
    "            pred_abnorm = torch.where(prob < thres, True, False)\n",
    "            correct += (pred_abnorm == abnorm).sum().item()\n",
    "            total += abnorm.size(0)\n",
    "        classify_net_acc = correct/total\n",
    "        if best_classify_net_acc < classify_net_acc:\n",
    "            best_classify_net_acc = classify_net_acc\n",
    "            print(\"Test... current best accuracy is {:.2f}%\".format(100 * best_classify_net_acc))\n",
    "print(\"Best accuracy: {:.2f}%\".format(100 * best_classify_net_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c2743f-cd5f-440a-88fa-06fab918c177",
   "metadata": {},
   "source": [
    "## 建構 AutoEncoder 的網路（第二小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73733600-a2a5-444d-907a-6e01437d3c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, ctx_size):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.img_size = TARGET_SIZE\n",
    "        h, w = self.img_size\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            FullyConnect(w * h, 256, nn.SiLU()),\n",
    "            FullyConnect(256, 128, nn.SiLU()),\n",
    "            FullyConnect(128, ctx_size, nn.SiLU()),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            FullyConnect(ctx_size, 128, nn.SiLU()),\n",
    "            FullyConnect(128, 256, nn.SiLU()),\n",
    "            FullyConnect(256, w * h, nn.Sigmoid())\n",
    "        )\n",
    "\n",
    "    def encode(self, img):\n",
    "        b, _, _, _ = img.shape\n",
    "        h, w = self.img_size\n",
    "        img = torch.reshape(img, (b, h * w))\n",
    "        ctx = self.encoder(img)\n",
    "        return ctx\n",
    "\n",
    "    def decode(self, ctx):\n",
    "        b, _ = ctx.shape\n",
    "        h, w = self.img_size\n",
    "        img = self.decoder(ctx)\n",
    "        img = torch.reshape(img, (b, 1, h, w))\n",
    "        return img\n",
    "\n",
    "    def forward(self, x):\n",
    "        ctx = self.encode(x)\n",
    "        x = self.decode(ctx)\n",
    "        return x\n",
    "\n",
    "def train_auto_encoder(net, device, loader):\n",
    "    net = net.to(device)\n",
    "    net.train()\n",
    "    bce_loss = nn.BCELoss()\n",
    "    opt = optim.Adam(net.parameters(),\n",
    "                     lr=0.002,\n",
    "                     weight_decay=0.0)\n",
    "\n",
    "    running_loss = list()\n",
    "\n",
    "    for e in range(200):\n",
    "        for imgs, _ in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            opt.zero_grad()\n",
    "            loss = bce_loss(net(imgs), imgs)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            running_loss.append(loss.item())\n",
    "            if len(running_loss) > 500:\n",
    "                running_loss.pop(0)\n",
    "        if (e+1) % 20 == 0:\n",
    "            print(\"epoch {} -> loss: {:.4f}\".format(\n",
    "                      e+1, sum(running_loss)/len(running_loss)))\n",
    "    return net\n",
    "\n",
    "def compute_threshold(net, device, loader, factor=1.2):\n",
    "    net.eval()\n",
    "    bce_loss_without_reduction = nn.BCELoss(reduction='none')\n",
    "    item_loss = torch.zeros(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, _ in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            loss = bce_loss_without_reduction(net(imgs), imgs)\n",
    "            loss = torch.flatten(loss, start_dim=1)\n",
    "            loss = torch.mean(loss, dim=1)\n",
    "            item_loss = torch.cat((item_loss, loss), 0)\n",
    "\n",
    "    item_loss = item_loss.detach().cpu().numpy()\n",
    "    mean = np.mean(item_loss)\n",
    "    std = np.std(item_loss)\n",
    "    thres = mean + factor * std\n",
    "    return thres\n",
    "\n",
    "def compute_accuracy(net, device, loader, thres):\n",
    "    bce_loss_without_reduction = nn.BCELoss(reduction='none')\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for imgs, abnorm in loader:\n",
    "            imgs, abnorm = imgs.to(device), abnorm.to(device)\n",
    "            loss = bce_loss_without_reduction(net(imgs), imgs)\n",
    "            loss = torch.flatten(loss, start_dim=1)\n",
    "            loss = torch.mean(loss, dim=1)\n",
    "            pred_abnorm = torch.where(loss > thres, True, False)\n",
    "            correct += (pred_abnorm == abnorm).sum().item()\n",
    "            total += abnorm.size(0)\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155784e9-abde-4218-9053-4c9907214fd6",
   "metadata": {},
   "source": [
    "## 訓練 AutoEncoder 的網路（第二小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff32ba0f-03be-4922-85bf-767d6a4ac543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 -> loss: 0.1449\n",
      "epoch 40 -> loss: 0.1405\n",
      "epoch 60 -> loss: 0.1377\n",
      "epoch 80 -> loss: 0.1371\n",
      "epoch 100 -> loss: 0.1366\n",
      "epoch 120 -> loss: 0.1370\n",
      "epoch 140 -> loss: 0.1364\n",
      "epoch 160 -> loss: 0.1339\n",
      "epoch 180 -> loss: 0.1350\n",
      "epoch 200 -> loss: 0.1363\n"
     ]
    }
   ],
   "source": [
    "ae_net = AutoEncoder(2)\n",
    "ae_net = ae_net.to(device)\n",
    "ae_net = train_auto_encoder(ae_net, device, t_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2ca8ea-e375-4e40-a611-b8c863b869fc",
   "metadata": {},
   "source": [
    "## 測試 AutoEncoder 的檢測性能（第二小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "577de0ca-6c42-4f00-8278-e4f9f3ac5d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test... current best accuracy is 79.10%\n",
      "Test... current best accuracy is 81.14%\n",
      "Test... current best accuracy is 83.24%\n",
      "Test... current best accuracy is 85.09%\n",
      "Test... current best accuracy is 86.14%\n",
      "Test... current best accuracy is 86.96%\n",
      "Best accuracy: 86.96%\n"
     ]
    }
   ],
   "source": [
    "best_ae_net_acc = 0\n",
    "for i in range(30):\n",
    "    factor = i/20\n",
    "    thres = compute_threshold(ae_net, device, t_loader, factor)\n",
    "    ae_net_acc = compute_accuracy(ae_net, device, a_loader, thres)\n",
    "    if best_ae_net_acc > ae_net_acc:\n",
    "        break\n",
    "    best_ae_net_acc = ae_net_acc\n",
    "    print(\"Test... current best accuracy is {:.2f}%\".format(100 * best_ae_net_acc))\n",
    "print(\"Best accuracy: {:.2f}%\".format(100 * best_ae_net_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b234de-1403-4e8e-9a55-01d7af8d271d",
   "metadata": {},
   "source": [
    "## 建構 Denoising AutoEncoder 的網路（第三小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ada95a6-a956-4bf0-8b20-0247ebe4601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoisingAutoEncoder(AutoEncoder):\n",
    "    def __init__(self, ctx_size):\n",
    "        super(DenoisingAutoEncoder, self).__init__(ctx_size)\n",
    "\n",
    "    def add_noise(self, img, noise_factor=0.25):\n",
    "        noise = torch.randn_like(img) * noise_factor\n",
    "        noisy_img = img + noise\n",
    "        return torch.clamp(noisy_img, 0., 1.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.add_noise(x)\n",
    "        ctx = self.encode(x)\n",
    "        x = self.decode(ctx)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a0d281-7b62-48d4-9e43-6798df9faf02",
   "metadata": {},
   "source": [
    "## 訓練 Denoising AutoEncoder 的網路（第三小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94cc920a-6a04-4699-b460-80bfd7f09da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 -> loss: 0.2350\n",
      "epoch 40 -> loss: 0.2349\n",
      "epoch 60 -> loss: 0.2349\n",
      "epoch 80 -> loss: 0.2351\n",
      "epoch 100 -> loss: 0.2350\n",
      "epoch 120 -> loss: 0.2350\n",
      "epoch 140 -> loss: 0.2345\n",
      "epoch 160 -> loss: 0.2350\n",
      "epoch 180 -> loss: 0.2350\n",
      "epoch 200 -> loss: 0.2351\n"
     ]
    }
   ],
   "source": [
    "dae_net = DenoisingAutoEncoder(2)\n",
    "dae_net = dae_net.to(device)\n",
    "dae_net = train_auto_encoder(dae_net, device, t_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf349544-6594-492c-be1f-43c5f3d8b345",
   "metadata": {},
   "source": [
    "## 測試 Denoising AutoEncoder 的檢測性能（第三小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16803b62-f83a-458f-9945-06ee05adc0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test... current best accuracy is 69.33%\n",
      "Best accuracy: 69.33%\n"
     ]
    }
   ],
   "source": [
    "best_dae_net_acc = 0\n",
    "for i in range(30):\n",
    "    factor = i/20\n",
    "    thres = compute_threshold(dae_net, device, t_loader, factor)\n",
    "    dae_net_acc = compute_accuracy(dae_net, device, a_loader, thres)\n",
    "    if best_dae_net_acc > dae_net_acc:\n",
    "        break\n",
    "    best_dae_net_acc = dae_net_acc\n",
    "    print(\"Test... current best accuracy is {:.2f}%\".format(100 * best_dae_net_acc))\n",
    "print(\"Best accuracy: {:.2f}%\".format(100 * best_dae_net_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30be95be-b008-420b-85ea-406d864710db",
   "metadata": {},
   "source": [
    "## 建構 Variational AutoEncoder 的網路（第四小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad1a9041-2206-4528-be00-1d6e4814d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(nn.Module):\n",
    "    def __init__(self, ctx_size):\n",
    "        super(VariationalAutoEncoder, self).__init__()\n",
    "        self.img_size = TARGET_SIZE\n",
    "        self.ctx_size = ctx_size\n",
    "        h, w = self.img_size\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            FullyConnect(w * h, 256, nn.SiLU()),\n",
    "            FullyConnect(256, 128, nn.SiLU()),\n",
    "            FullyConnect(128, 2 * ctx_size),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            FullyConnect(ctx_size, 128, nn.SiLU()),\n",
    "            FullyConnect(128, 256, nn.SiLU()),\n",
    "            FullyConnect(256, w * h, nn.Sigmoid())\n",
    "        )\n",
    "    \n",
    "    def encode(self, img):\n",
    "        b, _, _, _ = img.shape\n",
    "        h, w = self.img_size\n",
    "        img = torch.reshape(img, (b, h * w))\n",
    "        x = self.encoder(img)\n",
    "        mu, logvar = torch.split(x, self.ctx_size, dim=1)\n",
    "        return self.reparameterize(mu, logvar)\n",
    "\n",
    "    def decode(self, ctx):\n",
    "        b, _ = ctx.shape\n",
    "        h, w = self.img_size\n",
    "        img = self.decoder(ctx)\n",
    "        img = torch.reshape(img, (b, 1, h, w))\n",
    "        return img\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def encode_with_others(self, img):\n",
    "        b, _, _, _ = img.shape\n",
    "        h, w = self.img_size\n",
    "        img = torch.reshape(img, (b, h * w))\n",
    "        x = self.encoder(img)\n",
    "        mu, logvar = torch.split(x, self.ctx_size, dim=1)\n",
    "        return self.reparameterize(mu, logvar), mu, logvar\n",
    "\n",
    "    def forward_with_others(self, x):\n",
    "        input = x\n",
    "        x, mu, logvar = self.encode_with_others(x)\n",
    "        x = self.decode(x)\n",
    "        return x, mu, logvar\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        x, mu, logvar = self.encode_with_others(x)\n",
    "        x = self.decode(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0d3c14-a056-4b28-aa7f-dc0f77c34c12",
   "metadata": {},
   "source": [
    "## 訓練 Variational AutoEncoder 的網路（第四小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2abb875f-9bf9-44c3-8922-91e462a97521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 -> loss: 0.1375\n",
      "epoch 40 -> loss: 0.1344\n",
      "epoch 60 -> loss: 0.1325\n",
      "epoch 80 -> loss: 0.1309\n",
      "epoch 100 -> loss: 0.1296\n",
      "epoch 120 -> loss: 0.1289\n",
      "epoch 140 -> loss: 0.1275\n",
      "epoch 160 -> loss: 0.1265\n",
      "epoch 180 -> loss: 0.1260\n",
      "epoch 200 -> loss: 0.1255\n"
     ]
    }
   ],
   "source": [
    "def train_variational_auto_encoder(net, device, loader, loss_type=\"all\"):\n",
    "    assert loss_type in [\"all\", \"bce\", \"kld\"]\n",
    "    net = net.to(device)\n",
    "    net.train()\n",
    "    bce_loss = nn.BCELoss()\n",
    "    opt = optim.Adam(net.parameters(),\n",
    "                     lr=0.002,\n",
    "                     weight_decay=0.0)\n",
    "\n",
    "    running_loss = list()\n",
    "\n",
    "    for e in range(200):\n",
    "        for imgs, _ in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            opt.zero_grad()\n",
    "            output, mu, logvar = net.forward_with_others(imgs)\n",
    "\n",
    "            bce = bce_loss(output, imgs)\n",
    "            kld = torch.mean(-0.5 * torch.sum(1 + logvar - mu ** 2 - logvar.exp(), dim = 1), dim = 0)\n",
    "            if loss_type == \"all\":\n",
    "                loss = bce + kld\n",
    "            elif loss_type == \"bce\":\n",
    "                loss = bce\n",
    "            elif loss_type == \"kld\":\n",
    "                loss = kld\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            running_loss.append(loss.item())\n",
    "            if len(running_loss) > 500:\n",
    "                running_loss.pop(0)\n",
    "        if (e+1) % 20 == 0:\n",
    "            print(\"epoch {} -> loss: {:.4f}\".format(\n",
    "                      e+1, sum(running_loss)/len(running_loss)))\n",
    "    return net\n",
    "\n",
    "vae_net = VariationalAutoEncoder(2)\n",
    "vae_net = vae_net.to(device)\n",
    "vae_net = train_variational_auto_encoder(vae_net, device, t_loader, \"bce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e57d38-f2b6-49d8-b7fe-2d0473682ba6",
   "metadata": {},
   "source": [
    "## 測試 Variational AutoEncoder 的檢測性能（第四小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd1a8548-cd82-4ff7-8043-4c16b73147bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test... current best accuracy is 77.41%\n",
      "Test... current best accuracy is 78.07%\n",
      "Test... current best accuracy is 79.21%\n",
      "Test... current best accuracy is 80.06%\n",
      "Test... current best accuracy is 80.25%\n",
      "Test... current best accuracy is 80.35%\n",
      "Test... current best accuracy is 83.00%\n",
      "Test... current best accuracy is 83.56%\n",
      "Test... current best accuracy is 88.49%\n",
      "Best accuracy: 88.49%\n"
     ]
    }
   ],
   "source": [
    "best_vae_net_acc = 0\n",
    "for i in range(30):\n",
    "    factor = i/20\n",
    "    thres = compute_threshold(vae_net, device, t_loader, factor)\n",
    "    vae_net_acc = compute_accuracy(vae_net, device, a_loader, thres)\n",
    "    if best_vae_net_acc > vae_net_acc:\n",
    "        continue\n",
    "    best_vae_net_acc = vae_net_acc\n",
    "    print(\"Test... current best accuracy is {:.2f}%\".format(100 * best_vae_net_acc))\n",
    "print(\"Best accuracy: {:.2f}%\".format(100 * best_vae_net_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52b103d-4c35-4c6e-898c-96763cba6243",
   "metadata": {},
   "source": [
    "## 使用 Isolated Forest method 對 Variational AutoEncoder 的結果做異常檢測（第五小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ddcce0f-0968-42d4-8bf1-a478e17799a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 -> loss: 0.2351\n",
      "epoch 40 -> loss: 0.2351\n",
      "epoch 60 -> loss: 0.2347\n",
      "epoch 80 -> loss: 0.2349\n",
      "epoch 100 -> loss: 0.2350\n",
      "epoch 120 -> loss: 0.2352\n",
      "epoch 140 -> loss: 0.2351\n",
      "epoch 160 -> loss: 0.2349\n",
      "epoch 180 -> loss: 0.2351\n",
      "epoch 200 -> loss: 0.2349\n",
      "Accuracy: 44.94%\n"
     ]
    }
   ],
   "source": [
    "def train_isolation_forest(net, device, loader):\n",
    "    net.eval()\n",
    "    ctx_items = torch.zeros(0).to(device)\n",
    "    abnorm_tag = torch.zeros((0), dtype=torch.bool).to(device)\n",
    "    with torch.no_grad():\n",
    "        num_abnorms = 0\n",
    "        for imgs, abnorm in loader:\n",
    "            imgs, abnorm = imgs.to(device), abnorm.to(device)\n",
    "            ctx = net.encode(imgs)\n",
    "            ctx_items = torch.cat((ctx_items, ctx), 0)\n",
    "            abnorm_tag = torch.cat((abnorm_tag, abnorm), 0)\n",
    "            num_abnorms += (abnorm == True).sum().item()\n",
    "\n",
    "    ctx_items = ctx_items.detach().cpu().numpy()\n",
    "    abnorm_tag = abnorm_tag.detach().cpu().numpy()\n",
    "    num_items = ctx_items.shape[0]\n",
    "\n",
    "    if num_abnorms > 0.2 * num_items:\n",
    "        target_num_abnorms = 0.25 * (num_items - num_abnorms)\n",
    "        num_abnorms = 0\n",
    "        new_ctx_items = list()\n",
    "        for c, tag in zip(ctx_items, abnorm_tag):\n",
    "            if tag == True:\n",
    "                if num_abnorms < target_num_abnorms:\n",
    "                    new_ctx_items.append(c)\n",
    "                    num_abnorms += 1\n",
    "            else:\n",
    "                new_ctx_items.append(c)\n",
    "        ctx_items = np.array(new_ctx_items)\n",
    "        num_items = ctx_items.shape[0]\n",
    "    clf = IsolationForest(n_estimators=40,\n",
    "                          contamination=num_abnorms/num_items,\n",
    "                          max_samples=round(0.9 * num_items),\n",
    "                          random_state=99)\n",
    "    clf.fit(ctx_items)\n",
    "    return clf\n",
    "\n",
    "def compute_clf_accuracy(net, device, loader, clf):\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for imgs, abnorm in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            ctx = net.encode(imgs)\n",
    "            ctx = ctx.detach().cpu().numpy()\n",
    "    \n",
    "            abnorm = abnorm.detach().cpu().numpy()\n",
    "            pred = clf.predict(ctx)\n",
    "            pred = np.where(pred == -1, True, False)\n",
    "\n",
    "            total += abnorm.shape[0]\n",
    "            correct += (pred == abnorm).sum()\n",
    "    return correct/total\n",
    "\n",
    "vae_net_for_clf = VariationalAutoEncoder(4)\n",
    "vae_net_for_clf = vae_net_for_clf.to(device)\n",
    "vae_net_for_clf = train_variational_auto_encoder(vae_net_for_clf, device, t_loader, \"all\")\n",
    "\n",
    "clf = train_isolation_forest(vae_net_for_clf, device, r_loader)\n",
    "best_clf_acc = compute_clf_accuracy(vae_net_for_clf, device, a_loader, clf)\n",
    "print(\"Accuracy: {:.2f}%\".format(100 * best_clf_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
