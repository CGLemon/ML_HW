{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef1f8aeb-c692-4ac3-91cc-25a128f4a03f",
   "metadata": {},
   "source": [
    "## 導入所有必要的程式庫和宣告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4ef0833-9ab0-4082-9cd5-b6269a771092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "torchvision.disable_beta_transforms_warning()\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import alexnet, AlexNet_Weights\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "TARGET_SIZE = (28, 28)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409ac14b-e331-4e2a-b66a-86738a79b0b7",
   "metadata": {},
   "source": [
    "## 載入 MNIST 訓練資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a301a07b-5b55-415c-8fc4-d4a70a06680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Resize(size=TARGET_SIZE)])\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "class SubDataset(Dataset):\n",
    "    LABEL_TO_INDEX = {\n",
    "        1: 0, 3: 1, 5: 2, 7: 3\n",
    "    }\n",
    "    INDEX_TO_LABEL = [\n",
    "        1, 3, 5, 7\n",
    "    ]\n",
    "\n",
    "    def __init__(self, full_dataset):\n",
    "        self._data_pair = list()\n",
    "        for data in full_dataset:\n",
    "            img, label = data\n",
    "            if label in self.LABEL_TO_INDEX.keys():\n",
    "                self._data_pair.append((img, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data_pair)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self._data_pair[idx]\n",
    "        return img, self.LABEL_TO_INDEX[label]\n",
    "\n",
    "class AbnormDataset(Dataset):\n",
    "    NORM_LABEL = [\n",
    "        1, 3, 5, 7\n",
    "    ]\n",
    "\n",
    "    def __init__(self, full_dataset):\n",
    "        self._data_pair = list()\n",
    "        for data in full_dataset:\n",
    "            img, label = data\n",
    "            self._data_pair.append((img, label not in self.NORM_LABEL))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data_pair)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, abnorm = self._data_pair[idx]\n",
    "        return img, abnorm\n",
    "\n",
    "t_subset = SubDataset(trainset)\n",
    "t_loader = torch.utils.data.DataLoader(t_subset, batch_size=64, shuffle=True)\n",
    "v_subset = SubDataset(testset)\n",
    "v_loader = torch.utils.data.DataLoader(v_subset, batch_size=64, shuffle=True)\n",
    "r_dataset = AbnormDataset(trainset)\n",
    "r_loader = torch.utils.data.DataLoader(r_dataset, batch_size=64, shuffle=True)\n",
    "a_dataset = AbnormDataset(testset)\n",
    "a_loader = torch.utils.data.DataLoader(a_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1caf847-b576-495b-a4f2-2c98ebdc959e",
   "metadata": {},
   "source": [
    "## 建構用於分類網路（第一小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4082ac2-2df9-46eb-9034-dce6428c5763",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnect(nn.Module):\n",
    "    def __init__(self, in_size,\n",
    "                       out_size,\n",
    "                       activation=None):\n",
    "        super().__init__()\n",
    "        self.act = activation\n",
    "        self.linear = nn.Linear(\n",
    "            in_size,\n",
    "            out_size,\n",
    "            bias=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        if not self.act is None:\n",
    "            x = self.act(x)\n",
    "        return x\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels,\n",
    "                       out_channels,\n",
    "                       kernel_size,\n",
    "                       activation=None):\n",
    "        super().__init__()\n",
    "        self.act = activation\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            padding=\"same\",\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(\n",
    "            out_channels,\n",
    "            eps=1e-5\n",
    "        )\n",
    "        nn.init.kaiming_normal_(self.conv.weight,\n",
    "                                mode=\"fan_out\",\n",
    "                                nonlinearity=\"relu\")\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        if not self.act is None:\n",
    "            x = self.act(x)\n",
    "        return x\n",
    "\n",
    "class ClassifyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClassifyNetwork, self).__init__()\n",
    "        self.img_size = TARGET_SIZE\n",
    "\n",
    "        self.body = nn.Sequential(\n",
    "            ConvBlock(1, 32, 7, nn.SiLU()),\n",
    "            ConvBlock(32, 32, 3, nn.SiLU()),\n",
    "            ConvBlock(32, 2, 3, nn.SiLU())\n",
    "        )\n",
    "\n",
    "        h, w = self.img_size\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            FullyConnect(h * w * 2, 128, nn.SiLU()),\n",
    "            nn.Dropout(p=0.1),\n",
    "            FullyConnect(128, 4),\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.body(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def get_prob(self, x):\n",
    "        x = self.forward(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc95aaf7-5565-49f9-afc2-75bce89c3aaf",
   "metadata": {},
   "source": [
    "## 訓練 MNIST 分類網路（第一小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9bc0645-6306-4a70-9654-7672332ccefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 -> loss: 0.1040, acc: 99.19%\n",
      "epoch 2 -> loss: 0.0270, acc: 99.41%\n",
      "epoch 3 -> loss: 0.0183, acc: 99.29%\n",
      "epoch 4 -> loss: 0.0132, acc: 99.56%\n",
      "epoch 5 -> loss: 0.0118, acc: 99.41%\n",
      "epoch 6 -> loss: 0.0113, acc: 99.68%\n",
      "epoch 7 -> loss: 0.0097, acc: 99.66%\n",
      "epoch 8 -> loss: 0.0079, acc: 99.75%\n",
      "epoch 9 -> loss: 0.0089, acc: 99.58%\n",
      "epoch 10 -> loss: 0.0070, acc: 99.61%\n"
     ]
    }
   ],
   "source": [
    "classify_net = ClassifyNetwork()\n",
    "classify_net = classify_net.to(device)\n",
    "\n",
    "cross_entroy = nn.CrossEntropyLoss()\n",
    "opt = optim.SGD(classify_net.parameters(),\n",
    "                lr=0.01,\n",
    "                momentum=0.9,\n",
    "                nesterov=True,\n",
    "                weight_decay=0.001)\n",
    "\n",
    "running_loss = list()\n",
    "for e in range(10):\n",
    "    classify_net.train()\n",
    "    for imgs, labels in t_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        opt.zero_grad()\n",
    "        loss = cross_entroy(classify_net(imgs), labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        running_loss.append(loss.item())\n",
    "        if len(running_loss) > 500:\n",
    "            running_loss.pop(0)\n",
    "\n",
    "    classify_net.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for imgs, labels in v_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = classify_net(imgs)\n",
    "            _, pred = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (pred == labels).sum().item()\n",
    "        print(\"epoch {} -> loss: {:.4f}, acc: {:.2f}%\".format(\n",
    "                  e+1, sum(running_loss)/len(running_loss), 100.0 * correct/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b413b060-7da7-4056-b5c3-c953c111d0c7",
   "metadata": {},
   "source": [
    "## 測試分類網路的檢測性能（第一小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f383ed8b-fe2b-412a-9a03-3d40b5a01ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test... current best accuracy is 83.75%\n",
      "Test... current best accuracy is 83.76%\n",
      "Test... current best accuracy is 83.77%\n",
      "Test... current best accuracy is 83.79%\n",
      "Test... current best accuracy is 83.84%\n",
      "Test... current best accuracy is 83.88%\n",
      "Test... current best accuracy is 83.92%\n",
      "Test... current best accuracy is 83.97%\n",
      "Test... current best accuracy is 83.99%\n",
      "Test... current best accuracy is 84.04%\n",
      "Test... current best accuracy is 84.09%\n",
      "Test... current best accuracy is 84.13%\n",
      "Test... current best accuracy is 84.15%\n",
      "Test... current best accuracy is 84.18%\n",
      "Test... current best accuracy is 84.24%\n",
      "Test... current best accuracy is 84.28%\n",
      "Test... current best accuracy is 84.29%\n",
      "Test... current best accuracy is 84.33%\n",
      "Test... current best accuracy is 84.39%\n",
      "Test... current best accuracy is 84.44%\n",
      "Test... current best accuracy is 84.50%\n",
      "Test... current best accuracy is 84.51%\n",
      "Test... current best accuracy is 84.55%\n",
      "Test... current best accuracy is 84.59%\n",
      "Test... current best accuracy is 84.64%\n",
      "Test... current best accuracy is 84.68%\n",
      "Test... current best accuracy is 84.69%\n",
      "Test... current best accuracy is 84.81%\n",
      "Test... current best accuracy is 84.84%\n",
      "Test... current best accuracy is 84.90%\n",
      "Test... current best accuracy is 84.94%\n",
      "Test... current best accuracy is 84.95%\n",
      "Test... current best accuracy is 85.03%\n",
      "Test... current best accuracy is 85.10%\n",
      "Test... current best accuracy is 85.13%\n",
      "Test... current best accuracy is 85.18%\n",
      "Test... current best accuracy is 85.28%\n",
      "Test... current best accuracy is 85.33%\n",
      "Test... current best accuracy is 85.37%\n",
      "Test... current best accuracy is 85.39%\n",
      "Test... current best accuracy is 85.45%\n",
      "Test... current best accuracy is 85.54%\n",
      "Test... current best accuracy is 85.59%\n",
      "Test... current best accuracy is 85.62%\n",
      "Test... current best accuracy is 85.66%\n",
      "Test... current best accuracy is 85.68%\n",
      "Test... current best accuracy is 85.78%\n",
      "Test... current best accuracy is 85.81%\n",
      "Test... current best accuracy is 85.86%\n",
      "Test... current best accuracy is 85.88%\n",
      "Test... current best accuracy is 85.94%\n",
      "Test... current best accuracy is 85.97%\n",
      "Test... current best accuracy is 86.03%\n",
      "Test... current best accuracy is 86.10%\n",
      "Test... current best accuracy is 86.15%\n",
      "Test... current best accuracy is 86.21%\n",
      "Test... current best accuracy is 86.23%\n",
      "Test... current best accuracy is 86.36%\n",
      "Test... current best accuracy is 86.44%\n",
      "Test... current best accuracy is 86.45%\n",
      "Test... current best accuracy is 86.53%\n",
      "Test... current best accuracy is 86.56%\n",
      "Test... current best accuracy is 86.61%\n",
      "Test... current best accuracy is 86.62%\n",
      "Test... current best accuracy is 86.72%\n",
      "Test... current best accuracy is 86.78%\n",
      "Test... current best accuracy is 86.89%\n",
      "Test... current best accuracy is 86.94%\n",
      "Test... current best accuracy is 86.98%\n",
      "Test... current best accuracy is 87.07%\n",
      "Test... current best accuracy is 87.15%\n",
      "Test... current best accuracy is 87.17%\n",
      "Test... current best accuracy is 87.31%\n",
      "Test... current best accuracy is 87.34%\n",
      "Test... current best accuracy is 87.42%\n",
      "Test... current best accuracy is 87.54%\n",
      "Test... current best accuracy is 87.62%\n",
      "Test... current best accuracy is 87.66%\n",
      "Test... current best accuracy is 87.75%\n",
      "Test... current best accuracy is 87.76%\n",
      "Test... current best accuracy is 87.87%\n",
      "Test... current best accuracy is 88.01%\n",
      "Test... current best accuracy is 88.13%\n",
      "Test... current best accuracy is 88.21%\n",
      "Test... current best accuracy is 88.30%\n",
      "Test... current best accuracy is 88.34%\n",
      "Test... current best accuracy is 88.43%\n",
      "Test... current best accuracy is 88.51%\n",
      "Test... current best accuracy is 88.59%\n",
      "Test... current best accuracy is 88.60%\n",
      "Best accuracy: 88.60%\n"
     ]
    }
   ],
   "source": [
    "classify_net.eval()\n",
    "best_classify_net_acc = 0\n",
    "for i in range(100):\n",
    "    thres = 0.995 + 0.005 * (i/100)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for imgs, abnorm in a_loader:\n",
    "            imgs, abnorm = imgs.to(device), abnorm.to(device)\n",
    "            prob, _ = torch.max(classify_net.get_prob(imgs), dim=1)\n",
    "            pred_abnorm = torch.where(prob < thres, True, False)\n",
    "            correct += (pred_abnorm == abnorm).sum().item()\n",
    "            total += abnorm.size(0)\n",
    "        classify_net_acc = correct/total\n",
    "        if best_classify_net_acc < classify_net_acc:\n",
    "            best_classify_net_acc = classify_net_acc\n",
    "            print(\"Test... current best accuracy is {:.2f}%\".format(100 * best_classify_net_acc))\n",
    "print(\"Best accuracy: {:.2f}%\".format(100 * best_classify_net_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c2743f-cd5f-440a-88fa-06fab918c177",
   "metadata": {},
   "source": [
    "## 建構 AutoEncoder 的網路（第二小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73733600-a2a5-444d-907a-6e01437d3c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, ctx_size):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.img_size = TARGET_SIZE\n",
    "        h, w = self.img_size\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            FullyConnect(w * h, 256, nn.SiLU()),\n",
    "            FullyConnect(256, 128, nn.SiLU()),\n",
    "            FullyConnect(128, ctx_size, nn.SiLU()),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            FullyConnect(ctx_size, 128, nn.SiLU()),\n",
    "            FullyConnect(128, 256, nn.SiLU()),\n",
    "            FullyConnect(256, w * h, nn.Sigmoid())\n",
    "        )\n",
    "\n",
    "    def encode(self, img):\n",
    "        b, _, _, _ = img.shape\n",
    "        h, w = self.img_size\n",
    "        img = torch.reshape(img, (b, h * w))\n",
    "        ctx = self.encoder(img)\n",
    "        return ctx\n",
    "\n",
    "    def decode(self, ctx):\n",
    "        b, _ = ctx.shape\n",
    "        h, w = self.img_size\n",
    "        img = self.decoder(ctx)\n",
    "        img = torch.reshape(img, (b, 1, h, w))\n",
    "        return img\n",
    "\n",
    "    def forward(self, x):\n",
    "        ctx = self.encode(x)\n",
    "        x = self.decode(ctx)\n",
    "        return x\n",
    "\n",
    "def train_auto_encoder(net, device, loader):\n",
    "    net = net.to(device)\n",
    "    net.train()\n",
    "    bce_loss = nn.BCELoss()\n",
    "    opt = optim.Adam(net.parameters(),\n",
    "                     lr=0.002,\n",
    "                     weight_decay=0.0)\n",
    "\n",
    "    running_loss = list()\n",
    "\n",
    "    for e in range(200):\n",
    "        for imgs, _ in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            opt.zero_grad()\n",
    "            loss = bce_loss(net(imgs), imgs)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            running_loss.append(loss.item())\n",
    "            if len(running_loss) > 500:\n",
    "                running_loss.pop(0)\n",
    "        if (e+1) % 20 == 0:\n",
    "            print(\"epoch {} -> loss: {:.4f}\".format(\n",
    "                      e+1, sum(running_loss)/len(running_loss)))\n",
    "    return net\n",
    "\n",
    "def compute_threshold(net, device, loader, factor=1.2):\n",
    "    net.eval()\n",
    "    bce_loss_without_reduction = nn.BCELoss(reduction='none')\n",
    "    item_loss = torch.zeros(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, _ in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            loss = bce_loss_without_reduction(net(imgs), imgs)\n",
    "            loss = torch.flatten(loss, start_dim=1)\n",
    "            loss = torch.mean(loss, dim=1)\n",
    "            item_loss = torch.cat((item_loss, loss), 0)\n",
    "\n",
    "    item_loss = item_loss.detach().cpu().numpy()\n",
    "    mean = np.mean(item_loss)\n",
    "    std = np.std(item_loss)\n",
    "    thres = mean + factor * std\n",
    "    return thres\n",
    "\n",
    "def compute_accuracy(net, device, loader, thres):\n",
    "    bce_loss_without_reduction = nn.BCELoss(reduction='none')\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for imgs, abnorm in loader:\n",
    "            imgs, abnorm = imgs.to(device), abnorm.to(device)\n",
    "            loss = bce_loss_without_reduction(net(imgs), imgs)\n",
    "            loss = torch.flatten(loss, start_dim=1)\n",
    "            loss = torch.mean(loss, dim=1)\n",
    "            pred_abnorm = torch.where(loss > thres, True, False)\n",
    "            correct += (pred_abnorm == abnorm).sum().item()\n",
    "            total += abnorm.size(0)\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155784e9-abde-4218-9053-4c9907214fd6",
   "metadata": {},
   "source": [
    "## 訓練 AutoEncoder 的網路（第二小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff32ba0f-03be-4922-85bf-767d6a4ac543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 -> loss: 0.1384\n",
      "epoch 40 -> loss: 0.1341\n",
      "epoch 60 -> loss: 0.1315\n",
      "epoch 80 -> loss: 0.1305\n",
      "epoch 100 -> loss: 0.1292\n",
      "epoch 120 -> loss: 0.1281\n",
      "epoch 140 -> loss: 0.1274\n",
      "epoch 160 -> loss: 0.1268\n",
      "epoch 180 -> loss: 0.1259\n",
      "epoch 200 -> loss: 0.1257\n"
     ]
    }
   ],
   "source": [
    "ae_net = AutoEncoder(2)\n",
    "ae_net = ae_net.to(device)\n",
    "ae_net = train_auto_encoder(ae_net, device, t_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2ca8ea-e375-4e40-a611-b8c863b869fc",
   "metadata": {},
   "source": [
    "## 測試 AutoEncoder 的檢測性能（第二小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "577de0ca-6c42-4f00-8278-e4f9f3ac5d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test... current best accuracy is 77.27%\n",
      "Test... current best accuracy is 77.83%\n",
      "Test... current best accuracy is 78.37%\n",
      "Test... current best accuracy is 78.83%\n",
      "Test... current best accuracy is 79.41%\n",
      "Test... current best accuracy is 80.15%\n",
      "Test... current best accuracy is 80.72%\n",
      "Test... current best accuracy is 81.28%\n",
      "Test... current best accuracy is 82.01%\n",
      "Test... current best accuracy is 82.59%\n",
      "Test... current best accuracy is 83.25%\n",
      "Test... current best accuracy is 83.75%\n",
      "Test... current best accuracy is 84.42%\n",
      "Test... current best accuracy is 84.97%\n",
      "Test... current best accuracy is 85.41%\n",
      "Test... current best accuracy is 85.91%\n",
      "Test... current best accuracy is 86.18%\n",
      "Test... current best accuracy is 86.65%\n",
      "Test... current best accuracy is 87.02%\n",
      "Test... current best accuracy is 87.28%\n",
      "Test... current best accuracy is 87.47%\n",
      "Test... current best accuracy is 87.68%\n",
      "Test... current best accuracy is 87.89%\n",
      "Test... current best accuracy is 88.01%\n",
      "Test... current best accuracy is 88.04%\n",
      "Best accuracy: 88.04%\n"
     ]
    }
   ],
   "source": [
    "best_ae_net_acc = 0\n",
    "for i in range(30):\n",
    "    factor = i/20\n",
    "    thres = compute_threshold(ae_net, device, t_loader, factor)\n",
    "    ae_net_acc = compute_accuracy(ae_net, device, a_loader, thres)\n",
    "    if best_ae_net_acc > ae_net_acc:\n",
    "        break\n",
    "    best_ae_net_acc = ae_net_acc\n",
    "    print(\"Test... current best accuracy is {:.2f}%\".format(100 * best_ae_net_acc))\n",
    "print(\"Best accuracy: {:.2f}%\".format(100 * best_ae_net_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b234de-1403-4e8e-9a55-01d7af8d271d",
   "metadata": {},
   "source": [
    "## 建構 Denoising AutoEncoder 的網路（第三小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ada95a6-a956-4bf0-8b20-0247ebe4601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoisingAutoEncoder(AutoEncoder):\n",
    "    def __init__(self, ctx_size):\n",
    "        super(DenoisingAutoEncoder, self).__init__(ctx_size)\n",
    "\n",
    "    def add_noise(self, img, noise_factor=0.25):\n",
    "        noise = torch.randn_like(img) * noise_factor\n",
    "        noisy_img = img + noise\n",
    "        return torch.clamp(noisy_img, 0., 1.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.add_noise(x)\n",
    "        ctx = self.encode(x)\n",
    "        x = self.decode(ctx)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a0d281-7b62-48d4-9e43-6798df9faf02",
   "metadata": {},
   "source": [
    "## 訓練 Denoising AutoEncoder 的網路（第三小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94cc920a-6a04-4699-b460-80bfd7f09da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 -> loss: 0.1415\n",
      "epoch 40 -> loss: 0.1386\n",
      "epoch 60 -> loss: 0.1369\n",
      "epoch 80 -> loss: 0.1361\n",
      "epoch 100 -> loss: 0.1354\n",
      "epoch 120 -> loss: 0.1350\n",
      "epoch 140 -> loss: 0.1343\n",
      "epoch 160 -> loss: 0.1339\n",
      "epoch 180 -> loss: 0.1335\n",
      "epoch 200 -> loss: 0.1330\n"
     ]
    }
   ],
   "source": [
    "dae_net = DenoisingAutoEncoder(2)\n",
    "dae_net = dae_net.to(device)\n",
    "dae_net = train_auto_encoder(dae_net, device, t_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf349544-6594-492c-be1f-43c5f3d8b345",
   "metadata": {},
   "source": [
    "## 測試 Denoising AutoEncoder 的檢測性能（第三小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16803b62-f83a-458f-9945-06ee05adc0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test... current best accuracy is 78.77%\n",
      "Test... current best accuracy is 79.76%\n",
      "Test... current best accuracy is 80.24%\n",
      "Test... current best accuracy is 80.97%\n",
      "Test... current best accuracy is 81.49%\n",
      "Test... current best accuracy is 82.35%\n",
      "Test... current best accuracy is 83.04%\n",
      "Test... current best accuracy is 83.72%\n",
      "Test... current best accuracy is 84.26%\n",
      "Test... current best accuracy is 84.81%\n",
      "Test... current best accuracy is 85.32%\n",
      "Test... current best accuracy is 85.71%\n",
      "Test... current best accuracy is 86.10%\n",
      "Test... current best accuracy is 86.35%\n",
      "Test... current best accuracy is 86.42%\n",
      "Test... current best accuracy is 86.45%\n",
      "Test... current best accuracy is 86.60%\n",
      "Best accuracy: 86.60%\n"
     ]
    }
   ],
   "source": [
    "best_dae_net_acc = 0\n",
    "for i in range(30):\n",
    "    factor = i/20\n",
    "    thres = compute_threshold(dae_net, device, t_loader, factor)\n",
    "    dae_net_acc = compute_accuracy(dae_net, device, a_loader, thres)\n",
    "    if best_dae_net_acc > dae_net_acc:\n",
    "        break\n",
    "    best_dae_net_acc = dae_net_acc\n",
    "    print(\"Test... current best accuracy is {:.2f}%\".format(100 * best_dae_net_acc))\n",
    "print(\"Best accuracy: {:.2f}%\".format(100 * best_dae_net_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30be95be-b008-420b-85ea-406d864710db",
   "metadata": {},
   "source": [
    "## 建構 Variational AutoEncoder 的網路（第四小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad1a9041-2206-4528-be00-1d6e4814d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(nn.Module):\n",
    "    def __init__(self, ctx_size):\n",
    "        super(VariationalAutoEncoder, self).__init__()\n",
    "        self.img_size = TARGET_SIZE\n",
    "        self.ctx_size = ctx_size\n",
    "        h, w = self.img_size\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            FullyConnect(w * h, 256, nn.SiLU()),\n",
    "            FullyConnect(256, 128, nn.SiLU()),\n",
    "            FullyConnect(128, 2 * ctx_size),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            FullyConnect(ctx_size, 128, nn.SiLU()),\n",
    "            FullyConnect(128, 256, nn.SiLU()),\n",
    "            FullyConnect(256, w * h, nn.Sigmoid())\n",
    "        )\n",
    "    \n",
    "    def encode(self, img):\n",
    "        b, _, _, _ = img.shape\n",
    "        h, w = self.img_size\n",
    "        img = torch.reshape(img, (b, h * w))\n",
    "        x = self.encoder(img)\n",
    "        mu, logvar = torch.split(x, self.ctx_size, dim=1)\n",
    "        return self.reparameterize(mu, logvar)\n",
    "\n",
    "    def decode(self, ctx):\n",
    "        b, _ = ctx.shape\n",
    "        h, w = self.img_size\n",
    "        img = self.decoder(ctx)\n",
    "        img = torch.reshape(img, (b, 1, h, w))\n",
    "        return img\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def encode_with_others(self, img):\n",
    "        b, _, _, _ = img.shape\n",
    "        h, w = self.img_size\n",
    "        img = torch.reshape(img, (b, h * w))\n",
    "        x = self.encoder(img)\n",
    "        mu, logvar = torch.split(x, self.ctx_size, dim=1)\n",
    "        return self.reparameterize(mu, logvar), mu, logvar\n",
    "\n",
    "    def forward_with_others(self, x):\n",
    "        input = x\n",
    "        x, mu, logvar = self.encode_with_others(x)\n",
    "        x = self.decode(x)\n",
    "        return x, mu, logvar\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        x, mu, logvar = self.encode_with_others(x)\n",
    "        x = self.decode(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0d3c14-a056-4b28-aa7f-dc0f77c34c12",
   "metadata": {},
   "source": [
    "## 訓練 Variational AutoEncoder 的網路（第四小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2abb875f-9bf9-44c3-8922-91e462a97521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 -> loss: 0.1373\n",
      "epoch 40 -> loss: 0.1338\n",
      "epoch 60 -> loss: 0.1311\n",
      "epoch 80 -> loss: 0.1301\n",
      "epoch 100 -> loss: 0.1285\n",
      "epoch 120 -> loss: 0.1280\n",
      "epoch 140 -> loss: 0.1272\n",
      "epoch 160 -> loss: 0.1268\n",
      "epoch 180 -> loss: 0.1264\n",
      "epoch 200 -> loss: 0.1255\n"
     ]
    }
   ],
   "source": [
    "def train_variational_auto_encoder(net, device, loader, loss_type=\"all\"):\n",
    "    assert loss_type in [\"all\", \"bce\", \"kld\"]\n",
    "    net = net.to(device)\n",
    "    net.train()\n",
    "    bce_loss = nn.BCELoss()\n",
    "    opt = optim.Adam(net.parameters(),\n",
    "                     lr=0.002,\n",
    "                     weight_decay=0.0)\n",
    "\n",
    "    running_loss = list()\n",
    "\n",
    "    for e in range(200):\n",
    "        for imgs, _ in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            opt.zero_grad()\n",
    "            output, mu, logvar = net.forward_with_others(imgs)\n",
    "\n",
    "            output = torch.clamp(output, min=0.0001, max=0.9999)\n",
    "            bce = bce_loss(output, imgs)\n",
    "            kld = torch.mean(-0.5 * torch.sum(1 + logvar - mu ** 2 - logvar.exp(), dim = 1), dim = 0)\n",
    "            if loss_type == \"all\":\n",
    "                loss = bce + kld\n",
    "            elif loss_type == \"bce\":\n",
    "                loss = bce\n",
    "            elif loss_type == \"kld\":\n",
    "                loss = kld\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            running_loss.append(loss.item())\n",
    "            if len(running_loss) > 500:\n",
    "                running_loss.pop(0)\n",
    "        if (e+1) % 20 == 0:\n",
    "            print(\"epoch {} -> loss: {:.4f}\".format(\n",
    "                      e+1, sum(running_loss)/len(running_loss)))\n",
    "    return net\n",
    "\n",
    "vae_net = VariationalAutoEncoder(2)\n",
    "vae_net = vae_net.to(device)\n",
    "vae_net = train_variational_auto_encoder(vae_net, device, t_loader, \"bce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e57d38-f2b6-49d8-b7fe-2d0473682ba6",
   "metadata": {},
   "source": [
    "## 測試 Variational AutoEncoder 的檢測性能（第四小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd1a8548-cd82-4ff7-8043-4c16b73147bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test... current best accuracy is 77.59%\n",
      "Test... current best accuracy is 79.01%\n",
      "Test... current best accuracy is 80.27%\n",
      "Test... current best accuracy is 81.58%\n",
      "Test... current best accuracy is 83.68%\n",
      "Test... current best accuracy is 85.03%\n",
      "Test... current best accuracy is 86.07%\n",
      "Test... current best accuracy is 86.90%\n",
      "Test... current best accuracy is 87.82%\n",
      "Test... current best accuracy is 88.24%\n",
      "Test... current best accuracy is 88.52%\n",
      "Test... current best accuracy is 88.64%\n",
      "Test... current best accuracy is 88.78%\n",
      "Best accuracy: 88.78%\n"
     ]
    }
   ],
   "source": [
    "best_vae_net_acc = 0\n",
    "for i in range(30):\n",
    "    factor = i/20\n",
    "    thres = compute_threshold(vae_net, device, t_loader, factor)\n",
    "    vae_net_acc = compute_accuracy(vae_net, device, a_loader, thres)\n",
    "    if best_vae_net_acc > vae_net_acc:\n",
    "        continue\n",
    "    best_vae_net_acc = vae_net_acc\n",
    "    print(\"Test... current best accuracy is {:.2f}%\".format(100 * best_vae_net_acc))\n",
    "print(\"Best accuracy: {:.2f}%\".format(100 * best_vae_net_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52b103d-4c35-4c6e-898c-96763cba6243",
   "metadata": {},
   "source": [
    "## 直接使用 Isolated Forest method 做異常檢測（第五小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ddcce0f-0968-42d4-8bf1-a478e17799a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.14%\n"
     ]
    }
   ],
   "source": [
    "def train_isolation_forest(loader):\n",
    "    items = torch.zeros(0)\n",
    "    abnorm_tag = torch.zeros((0), dtype=torch.bool)\n",
    "    num_abnorms = 0\n",
    "    for imgs, abnorm in loader:\n",
    "        items = torch.cat((items, imgs.flatten(start_dim=1)), 0)\n",
    "        abnorm_tag = torch.cat((abnorm_tag, abnorm), 0)\n",
    "        num_abnorms += (abnorm == True).sum().item()\n",
    "    items = items.detach().cpu().numpy()\n",
    "    abnorm_tag = abnorm_tag.detach().cpu().numpy()\n",
    "    num_items = items.shape[0]\n",
    "\n",
    "    if num_abnorms > 0.2 * num_items:\n",
    "        target_num_abnorms = 0.25 * (num_items - num_abnorms)\n",
    "        num_abnorms = 0\n",
    "        new_items = list()\n",
    "        for c, tag in zip(items, abnorm_tag):\n",
    "            if tag == True:\n",
    "                if num_abnorms < target_num_abnorms:\n",
    "                    new_items.append(c)\n",
    "                    num_abnorms += 1\n",
    "            else:\n",
    "                new_items.append(c)\n",
    "        items = np.array(new_items)\n",
    "        num_items = items.shape[0]\n",
    "    clf = IsolationForest(n_estimators=40,\n",
    "                          contamination=num_abnorms/num_items,\n",
    "                          max_samples=round(0.9 * num_items),\n",
    "                          random_state=99)\n",
    "    clf.fit(items)\n",
    "    return clf\n",
    "\n",
    "def compute_clf_accuracy(loader, clf):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for imgs, abnorm in loader:\n",
    "        imgs = imgs.flatten(start_dim=1).detach().cpu().numpy()\n",
    "        abnorm = abnorm.detach().cpu().numpy()\n",
    "        pred = clf.predict(imgs)\n",
    "        pred = np.where(pred == -1, True, False)\n",
    "\n",
    "        total += abnorm.shape[0]\n",
    "        correct += (pred == abnorm).sum()\n",
    "    return correct/total\n",
    "\n",
    "clf = train_isolation_forest(r_loader)\n",
    "best_clf_acc = compute_clf_accuracy(a_loader, clf)\n",
    "print(\"Accuracy: {:.2f}%\".format(100 * best_clf_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f679f2-377d-41dd-99b8-a2c05efd0458",
   "metadata": {},
   "source": [
    "## 使用預訓練的網路配合 Isolated Forest method 做異常檢測（第六小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "166855bb-37a1-4ae3-932d-bdaba74cb04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.61%\n"
     ]
    }
   ],
   "source": [
    "def train_isolation_forest_with_net(net, device, loader):\n",
    "    net.eval()\n",
    "    prob_items = torch.zeros(0).to(device)\n",
    "    abnorm_tag = torch.zeros((0), dtype=torch.bool).to(device)\n",
    "    with torch.no_grad():\n",
    "        num_abnorms = 0\n",
    "        for imgs, abnorm in loader:\n",
    "            imgs, abnorm = imgs.to(device), abnorm.to(device)\n",
    "            prob = net.get_prob(imgs)\n",
    "            prob_items = torch.cat((prob_items, prob), 0)\n",
    "            abnorm_tag = torch.cat((abnorm_tag, abnorm), 0)\n",
    "            num_abnorms += (abnorm == True).sum().item()\n",
    "    prob_items = prob_items.detach().cpu().numpy()\n",
    "    abnorm_tag = abnorm_tag.detach().cpu().numpy()\n",
    "    num_items = prob_items.shape[0]\n",
    "\n",
    "    if num_abnorms > 0.2 * num_items:\n",
    "        target_num_abnorms = 0.25 * (num_items - num_abnorms)\n",
    "        num_abnorms = 0\n",
    "        new_prob_items = list()\n",
    "        for c, tag in zip(prob_items, abnorm_tag):\n",
    "            if tag == True:\n",
    "                if num_abnorms < target_num_abnorms:\n",
    "                    new_prob_items.append(c)\n",
    "                    num_abnorms += 1\n",
    "            else:\n",
    "                new_prob_items.append(c)\n",
    "        prob_items = np.array(new_prob_items)\n",
    "        num_items = prob_items.shape[0]\n",
    "    clf = IsolationForest(n_estimators=40,\n",
    "                          contamination=num_abnorms/num_items,\n",
    "                          max_samples=round(0.9 * num_items),\n",
    "                          random_state=99)\n",
    "    clf.fit(prob_items)\n",
    "    return clf\n",
    "\n",
    "def compute_clf_accuracy_with_net(net, device, loader, clf):\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for imgs, abnorm in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            prob = net.get_prob(imgs)\n",
    "            prob = prob.detach().cpu().numpy()\n",
    "\n",
    "            abnorm = abnorm.detach().cpu().numpy()\n",
    "            pred = clf.predict(prob)\n",
    "            pred = np.where(pred == -1, True, False)\n",
    "\n",
    "            total += abnorm.shape[0]\n",
    "            correct += (pred == abnorm).sum()\n",
    "    return correct/total\n",
    "    \n",
    "clf = train_isolation_forest_with_net(classify_net, device, r_loader)\n",
    "best_pretrained_clf_acc = compute_clf_accuracy_with_net(classify_net, device, a_loader, clf)\n",
    "print(\"Accuracy: {:.2f}%\".format(100 * best_pretrained_clf_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580c286a-f7e2-4103-a1a0-310c22ab2a7b",
   "metadata": {},
   "source": [
    "## 比較不同方法的最佳結果（第七小題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b488eb8-1e0f-4db3-98de-51b98585865f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 The classify net (第ㄧ小題) accuracy: 88.60%\n",
      "                                  The AutoEncoder (第二小題) accuracy: 88.04%\n",
      "                        The Denoising Autoencoder (第三小題) accuracy: 86.60%\n",
      "                  The Variational Autoencoder net (第四小題) accuracy: 88.78%\n",
      "                       The Isolated Forest method (第五小題) accuracy: 62.14%\n",
      "The Isolated Forest method with pre-trained model (第六小題) accuracy: 84.61%\n"
     ]
    }
   ],
   "source": [
    "print(\"                                 The classify net (第ㄧ小題) accuracy: {:.2f}%\".format(100 * best_classify_net_acc))\n",
    "print(\"                                  The AutoEncoder (第二小題) accuracy: {:.2f}%\".format(100 * best_ae_net_acc))\n",
    "print(\"                        The Denoising Autoencoder (第三小題) accuracy: {:.2f}%\".format(100 * best_dae_net_acc))\n",
    "print(\"                  The Variational Autoencoder net (第四小題) accuracy: {:.2f}%\".format(100 * best_vae_net_acc))\n",
    "print(\"                       The Isolated Forest method (第五小題) accuracy: {:.2f}%\".format(100 * best_clf_acc))\n",
    "print(\"The Isolated Forest method with pre-trained model (第六小題) accuracy: {:.2f}%\".format(100 * best_pretrained_clf_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864e08db-a086-4028-8829-bf46fe8ce5d4",
   "metadata": {},
   "source": [
    "在所有檢測方法中，只要調整好不同方法的閾值，所有使用神經網路輔助的方法，其準確度都差不多，唯有直接使用 Isolated Forest 方法的沒有使用神經網路，它的準確度相對較差。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
